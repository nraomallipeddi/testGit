---caap_spark_streaming_init.properties
#Spark properties
spark.eventLog.enabled = false
spark.eventLog.dir = /tmp
spark.scheduler.mode = FAIR
spark.memory.useLegacyMode = false
spark.yarn.executor.memoryOverhead = 4096
spark.network.timeout = 1200
spark.rpc.askTimeout = 1200
spark.rpc.lookupTimeout = 1200
spark.executor.heartbeatInterval = 30
spark.rpc.numRetries = 6
spark.task.maxFailures = 8
spark.sql.shuffle.partitions = 201
spark.sql.tungsten.enabled = true
spark.sql.ui.retainedExecutions = 10
spark.shuffle.manager = SORT
spark.hadoop.mapred.output.compress = false
spark.streaming.kafka.maxRatePerPartition = 1000
spark.broadcast.factor = org.apache.spark.broadcast.HttpBroadcastFactory
------------------------------
----caap_streaming.ini
[GLOBAL]
APP_NAME=CATENA
SPARK_MASTER=yarnclient
BASEDIR=/home/ebdpcedms/jobs/data_ingestion
EMAIL=_DS-DataMgmt-CEDM-DEV_@comcast.com
LOG_DIR=/home/ebdpcedms/jobs/data_ingestion/caap_spark_streaming_logs
LOG_RETAIN_DAYS=4
FAILED_RETRY_TIME=1
SPARK_INIT_FILE=/home/ebdpcedms/jobs/data_ingestion/ini/caap_spark_streaming_init.properties
EXECUTION_MODE=STREAMING
STREAMING_MODE=checkpoint
SPARK_MAJOR_VERSION=2
SPARK2_SUBMIT=/home/ebdpcedms/spark-2.2.0-bin-hadoop2.7/bin/spark-submit
SPARK_SUBMIT=spark-submit
QUEUE=catena_highsla
NO_OF_EXECUTERS=10
EXECUTOR_MEMORY=4G
EXECUTOR_CORES=1
DRIVER_MEMORY=4G
DRIVER_CLASS_NAME=com.comcast.meld.catena.core.Driver
SPARK2_DRIVER_JAR=/home/ebdpcedms/jobs/data_ingestion/jars/CAAP/catena-spark_2.11-1.0.jar
SPARK1_DRIVER_JAR=/home/ebdpcedms/jobs/data_ingestion/jars/CAAP/catena-spark1_2.10-1.0.jar
JAR_FILES=/home/ebdpcedms/jobs/data_ingestion/jars/common/config-1.3.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/spark-streaming-kafka-0-8-assembly_2.11-2.2.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/kafka_2.10-0.9.0.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/kafka-clients-0.9.0.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/json4s-jackson_2.11-3.3.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/json4s-core_2.11-3.3.0.jar
KAFKA_OFFSET_DEFAULT=largest
INPUT_TOPIC=caapkerberostest
MAX_MESSAGE_PER_PARTITION=1000
GROUP_ID=caap_team_g
ZOOKEEPER_CONNECT=ebdp-ch2-d033s.sys.comcast.net:2181,ebdp-ch2-d041s.sys.comcast.net:2181,ebdp-ch2-d043s.sys.comcast.net:2181
BOOTSTRAP_SERVERS=ebdp-ch2-d039s.sys.comcast.net:6667,ebdp-ch2-d050s.sys.comcast.net:6667
SECURITY_PROTOCOL=PLAINTEXTSASL
BATCH_INTERVAL=120
SPARK_UI=http://ebdp-ch2-c009s.sys.comcast.net:18081
AMBARI_LINK=http://ebdp-ch2-d033s.sys.comcast.net:8080
WIKI_LINK=https://wiki.sys.comcast.net/pages/viewpage.action?pageId=90870075
EMAIL_SEND_TO=narayanarao_mallipeddi@cable.comcast.com
EMAIL_SEND_CC=narayanarao_mallipeddi@cable.comcast.com
EMAIL_SEND_FROM=narayanarao_mallipeddi@cable.comcast.com
EMAIL_SEND_SMTPHOST=ulamailrelay.g.comcast.com

[CAAP_TEAM_G_STREAMING]
JOB_NAME=caap_team_g_streaming
OUTPUT_PATH=/db/catena/semantic/ctp/summary_ctp_orders/data
TABLE_NAME=SUMMARY_CTP_ORDERS
DB_NAME=JRNL
NUM_PARTITIONS=5

----------------
----spark_sql_wrapper.sh

#################################################################################
# v1.0 - 2014-01-03 - Initial creation
# CEDM - Spark sql wrapper to run the spark-submit with parameterization
#################################################################################
# This script is a wrapper for running Hive SQL files 
#
# Return Codes:
#    0   - sucessful execution
#    101 - a required command line input is either missing or invalid
#    102 - an unrecognized input switch was provided
#    103 - could not create temporary log file
#    104 - a required variable was not defined in the INI_FILE
#    105 - could not remove an old log file
#    106 - the Lock file (LOCK_FILE) is not writable
#    107 - the Lock file (LOCK_FILE) already exists.  A previous scheduled
#          processes is either still running or has failed mid-stream
#    112 - the Log Directory (LOG_DIR) does not exist
#    113 - the archive subdirectory of the Log Directory (LOG_DIR/archive) does not exist
#    114 - the Log Directory (LOG_DIR) is not writable
#    115 - the archive subdirectory of the Log Directory (LOG_DIR/archive) is not writable"
#    116 - could not remove LOCK_FILE after completion
#    117 - could not ceate LOCK_FILE after initializations
#    124 - INI file does not exist
#    125 - referenced SQL file does not exist
#    126 - SPARK returned a non-zero exit code
#
#################################################################################
# Define all the functions (in the future, potential to use fpath)
#set -x

function PROGRAM_USAGE(){
   echo
   echo "Usage:  ./spark_sql_wrapper.sh -i [INI_FILE] -s [INI_SECTION] -r [RUNTIME_INI]"
   echo "   -i is required.  This is the fully qualified (pathed) INI filename"
   echo "   -s is required.  This is the section in the INI file pertaining to"
   echo "                    the specific execution.  Values are case specific"
   echo "   -r is required.  This is the fully qualified Runtime INI filename(Job Specific)"
   echo
   echo "Example:"
   echo "   /full/path/spark_sql_wrapper.sh -i /full/path/something.ini -s SECTION -r /full/path/runtime.ini"
   echo
   echo "Return Codes:"
   echo "   0   - sucessful execution"
   echo "   101 - a required command line input is either missing or invalid"
   echo "   102 - an unrecognized input switch was provided"
   echo "   103 - could not create temporary log file"
   echo "   104 - a required variable was not defined in the INI_FILE"
   echo "   105 - could not remove an old log file"
   echo "   106 - the Lock file (LOCK_FILE) is not writable"
   echo "   107 - the Lock file (LOCK_FILE) already exists.  A previous scheduled"
   echo "         process is either still running or has failed mid-stream"
   echo "   112 - the Log Directory (LOG_DIR) does not exist"
   echo "   113 - the archive subdirectory of the Log Directory (LOG_DIR/archive) does not exist"
   echo "   114 - the Log Directory (LOG_DIR) is not writable"
   echo "   115 - the archive subdirectory of the Log Directory (LOG_DIR/archive) is not writable"
   echo "   116 - could not remove LOCK_FILE after SFTP completion"
   echo "   117 - could not ceate LOCK_FILE after initializations"
   echo "   124 - INI file does not exist"
   echo "   125 - referenced SQL file does not exist"
   echo "   126 - SPARK returned a non-zero exit code"
   echo
}   

function PARSE_INI(){
   # function inputs:
   # ${1} = fully qualified path to INI file
   # ${2} = name of the section

   eval `sed -e 's/[[:space:]]*\=[[:space:]]*/=/g' \
       -e 's/;.*$//' \
       -e 's/[[:space:]]*$//' \
       -e 's/^[[:space:]]*//' \
       -e "s/^\(.*\)=\([^\"']*\)$/\1=\"\2\"/" \
      < ${1} \
       | sed -n -e "/^\[${2}\]/,/^\s*\[/{/^[^;].*\=.*/p;}"`
}

function LOGGER(){
   # function inputs:
   # ${1} = message type (I, W, E)
   # ${2} = message to be logged
   #
   # The fields in the log message are dash (-) delimited, and are formatted as::
   #   field 1, srting(29) - timestamp in format:  YYYY-MM-DD HH:MI:SS.NNNNNNNNN 
   #   field 2, string(7), right padded with spaces - message type (INFO, WARNING, ERROR), 
   #   field 3, string(n) - descriptive message

   # default message type is Info
   MESSAGE_TYPE="INFO   "  

   if [ "${1}" = "W" ]
   then 
      MESSAGE_TYPE="WARNING"
   elif [ "${1}" = "E" ]
   then
     MESSAGE_TYPE="ERROR  "
   fi 

   echo `date "+%Y-%m-%d %H:%M:%S.%N"`-"${MESSAGE_TYPE}"-"${2}" | tee -a ${LOG_FILE}
}

function FAILURE(){
   # funtion inputs:
   # ${1} = the return code to exit with

   LOGGER E "Exiting with custom return code ${1}"

#mailx -s "${JOB_NAME} Failure" ${EMAIL} << EOF
#The ${JOB_NAME} Job has failed.  Please review the log file located at ${HOSTNAME}:${LOG_FILE} for more info.
#Here is the log generated during this run.

#`cat ${LOG_FILE}`
#EOF
   
   exit ${1}
}

function CHECK_NULL_VAR(){
   # function inputs:
   # ${1} = variable name
   # ${2} = variable value
   # ${3} = exit code to use when check fails
   #        if exit code is null, then do not fail

   LOGGER I "Begin - confirm ${1} has been set and has a value"
   if [ "X${2}" = "X" ]
   then
      if [ "X${3}" = "X" ]
      then
         LOGGER I "Value for ${1} is null (not an error condition)"
      else
         LOGGER E "********** ERROR **********"
         LOGGER E "A valid value for ${1} has not been set"
         FAILURE ${3};
      fi
   fi
   LOGGER I "${1} has been set to: ${2}"
   LOGGER I "End - confirm ${1} has been set and has a value"
}

function CHECK_FILE_WRITABLE(){
   # function inputs
   # ${1} = variable name
   # ${2} = variable value - fully qualified file name
   # ${3} = exit code to use when check fails
   #        if exit code is null, then do not fail

   # need a variable so we know if we need to RM our test file or not
   TEMP_TOUCH_FLAG=0

   LOGGER I "Begin - Check if ${1} file ${2} is writable"
   # first we have to check if the exists or not
   if ! [ -f ${2} ]
   then
      LOGGER I "Touching a temp file for writable validation"
      TEMP_TOUCH_FLAG=1
      touch ${2} > /dev/null 2>&1
   fi

   if ! [ -w ${2} ]
   then
      if [ "X${3}" = "X" ]
      then
         LOGGER W "${1} file ${2} is not writable (not an error condition)"
      else
         LOGGER E "********* ERROR **********"
         LOGGER E "${1} file ${2} is not wiritable"
         LOGGER E "Check the location and permissions"
         FAILURE ${3}
      fi
   fi

   if [ ${TEMP_TOUCH_FLAG} -ne 0 ]
   then
      LOGGER I "Removing the temp file used for writable validation"
      rm -f ${2} > /dev/null 2>&1
   fi

   LOGGER I "End - check if ${1} file ${2} is writable"
}

function CHECK_FILE_EXISTS(){
   # function inputs
   # ${1} = variable name
   # ${2} = varibale value - fully qualified file name
   # ${3} = exit code to use when check fails
   #        if exit code is null, then do not fail

   LOGGER I "Begin - Check if ${1} file ${2} exists"
   if ! [ -f ${2} ]
   then
      if [ "X${3}" = "X" ]
      then
         LOGGER W "${1} file ${2} does not exist (not an error condition)"
      else
         LOGGER E "********* ERROR **********"
         LOGGER E "${1} file ${2} does not exist"
         LOGGER E "Research cause of missing file, create or restore file if needed"
         FAILURE ${3}
      fi
   else
      LOGGER I "${1} file ${2} exists"
   fi
   LOGGER I "End - check if ${1} file ${2} exists"
}

# Done defining the functions
#################################################################################
# Start the logical processing

# Set some internal variables
PROGNAMELONG=`basename $0`                                      # name of the program, minus all of the path info
PROGNAMESHORT=`echo ${PROGNAMELONG} | sed 's/\.[^.]*$//'`       # name of the program, minus the extension
DATETIME=`date +%Y%m%d%H%M%S`                                   # datetime stamp yyyymmddhhmiss
CURPID=$$                              # PID of the current running process (knowing this helps with concurrency)


# Before we can initialize the dynamic log management, we have to parse the
# command line and INI to set all of the varibale options.  However, we do want
# to generate a log right immediately so that we can log the process of parsing
# the command line and INI (to be able to troubleshoot if those steps fail).
# We'll start by creating a temporary log in the current working directory.
# 
# Dynamic log management will be initialized only after a successful read of the 
# command line and INI options.  Once that is ready, we'll move the log out of
# current working directory and into the directory structrue defined in the INI.

TEMP_LOG_DIR=/tmp                                              # just the current working directory
TEMP_LOG_FILE=${TEMP_LOG_DIR}/${PROGNAMESHORT}_${DATETIME}_${CURPID}.log  # temporary log   
LOG_FILE=${TEMP_LOG_FILE}


# Now make sure that we can actually open and write the temporary log 
touch ${LOG_FILE} > /dev/null 2>&1
if ! [ -w ${LOG_FILE} ]; then
   echo "********** ERROR **********"
   echo "The temporary log file ${LOG_FILE} is not writeable"
   echo "Check the location and permissions and try again"
   FAILURE 103
fi

# OK, now we can start actually logging
LOGGER I "********** BEGIN PROCESS **********"
LOGGER I "Full command line: ${0} ${*}"
LOGGER I "The current unix pid is: ${CURPID}"

# Parse the command line options provided
LOGGER I "Begin - parse command line options with getopts"
while getopts :i:s:r: OPTIONS
do
   case ${OPTIONS} in
      i)  INI_FILE=${OPTARG};;
      s)  INI_SECTION=${OPTARG};;
      r)  RUNTIME_INI=${OPTARG};;
     \?)  LOGGER E "********** ERROR **********"
          LOGGER E "A required switch was missing, or an invalid switch was supplied"
          PROGRAM_USAGE
          FAILURE 102;; 
   esac
done
LOGGER I "End - parse command line options with getopts"


# check to ensure that the required -i switch for INI_FILE was provided
CHECK_NULL_VAR INI_FILE "${INI_FILE}" 101
# Confirm that the INI file specified actualy exists
CHECK_FILE_EXISTS INI_FILE "${INI_FILE}" 124

# check to ensure that the required -i switch for RUNTIME_INI was provided
CHECK_NULL_VAR RUNTIME_INI "${RUNTIME_INI}" 101
# Confirm that the INI file specified actualy exists
CHECK_FILE_EXISTS RUNTIME_INI "${RUNTIME_INI}" 124

# check to ensure that the required -s switch for INI_SECTION was provided
CHECK_NULL_VAR INI_SECTION "${INI_SECTION}" 101

# Source global parameters from the INI file
LOGGER I "Begin - read GLOBAL varibales from INI_FILE"
PARSE_INI ${INI_FILE} GLOBAL
LOGGER I "End - read GLOBAL varibales from INI_FILE"

# Source global parameters from the Runtime INI file
LOGGER I "Begin - read GLOBAL varibales from RUNTIME_INI"
PARSE_INI ${RUNTIME_INI} GLOBAL
LOGGER I "End - read GLOBAL varibales from RUNTIME_INI"

# source process-specific parameters from the INI file
LOGGER I "Begin - read process-specifc variables from INI_FILE for INI_SECTION"
PARSE_INI ${INI_FILE} ${INI_SECTION}
LOGGER I "End - read process-specific variables from INI_FILE for INI_SECTION"

# OK, now we need to validate that other expected variables have been set by the INI
# process, and that access to additional files or directories are as expected
CHECK_NULL_VAR EMAIL "${EMAIL}" 104
CHECK_NULL_VAR EXECUTION_MODE "${EXECUTION_MODE}" 104
CHECK_NULL_VAR SPARK_MAJOR_VERSION "${SPARK_MAJOR_VERSION}" 104
CHECK_NULL_VAR SPARK2_SUBMIT "${SPARK2_SUBMIT}" 104
CHECK_NULL_VAR SPARK_SUBMIT "${SPARK_SUBMIT}" 104
CHECK_NULL_VAR QUEUE "${QUEUE}" 104
CHECK_NULL_VAR NO_OF_EXECUTERS "${NO_OF_EXECUTERS}" 104
CHECK_NULL_VAR EXECUTOR_MEMORY "${EXECUTOR_MEMORY}" 104
CHECK_NULL_VAR EXECUTOR_CORES "${EXECUTOR_CORES}" 104
CHECK_NULL_VAR DRIVER_MEMORY "${DRIVER_MEMORY}" 104
CHECK_NULL_VAR SPARK2_DRIVER_JAR "${SPARK2_DRIVER_JAR}" 104
CHECK_NULL_VAR SPARK1_DRIVER_JAR "${SPARK1_DRIVER_JAR}" 104
CHECK_NULL_VAR DRIVER_CLASS_NAME "${DRIVER_CLASS_NAME}" 104
CHECK_NULL_VAR JAR_FILES "${JAR_FILES}" 104

CHECK_NULL_VAR LOG_DIR "${LOG_DIR}" 104

# Check to ensure the log directory exists
# note - we could also decide to attempt to create it within the script
# but my fear is that if the directory disappears, that there might be a
# larger issue occurring
if ! [ -d ${LOG_DIR} ]
then
   LOGGER E "********** ERROR **********"
   LOGGER E "The LOG_DIR ${LOG_DIR} does not exist"
   LOGGER E "Ensure that a valid directory with the proper access exists"
   FAILURE 112
fi

# check to confirm that the log directory is writable
CHECK_FILE_WRITABLE LOG_DIR_WRITABLE_CHECK ${LOG_DIR}/temp_${CURPID}.log 114

# Check to ensure the archive subdirectory of the log directory exists
if ! [ -d ${LOG_DIR}/archive ]
then
   LOGGER E "********** ERROR **********"
   LOGGER E "The archive subdirectory of LOG_DIR ${LOG_DIR} does not exist"
   LOGGER E "Ensure that a valid directory with the proper access exists"
   FAILURE 113
fi

# check to ensure that the archive subdirectory of the log directory is writable
CHECK_FILE_WRITABLE LOG_DIR_ARCHIVE_WRITABLE_CHECK ${LOG_DIR}/archive/temp_${CURPID}.log 115

CHECK_NULL_VAR JOB_NAME "${JOB_NAME}" 104

CHECK_NULL_VAR JOB_ID "${JOB_ID}" 104

#CHECK_NULL_VAR LOCK_FILE "${LOCK_FILE}" 104
# Intentionally not using the CHECK_FILE_EXIST function here
# In this case, I want the process to fail if the file does
# exist instead of failing when it does not
#if [ -f ${LOCK_FILE} ]
#then
#   LOGGER E "********** ERROR **********"
#   LOGGER E "The LOCK_FILE ${LOCK_FILE} already exists"
#   LOGGER E "This indicates that either the previously scheduled run of"
#   LOGGER E "the job is still running, or that the previous run of the job"
#   LOGGER E "has failed.  The root cause needs to be researched before"
#   LOGGER E "the process can be allowed to run, else you run the risk of"
#   LOGGER E "messing up the High Water Mark and perhaps missing files."
#   FAILURE 107
#fi
#CHECK_FILE_WRITABLE LOCK_FILE "${LOCK_FILE}" 106

# OK, now at this point all varaibles and parameters have been set
# so we'll switch the logging over to the correct log store.

LOGGER I "Begin - switching temporary log file ${TEMP_LOG_FILE} to managed structure"
LOG_FILE=${LOG_DIR}/${PROGNAMESHORT}_${JOB_NAME}_${DATETIME}_${CURPID}.log
mv -f ${TEMP_LOG_FILE} ${LOG_FILE} 
LOGGER I "The LOG_FILE is now being written to ${LOG_FILE}"
LOGGER I "End - switching temporary LOG to managed structure"

# check optional varibale GUNZIP_TARGET
if [ "${GUNZIP_TARGET}" = "YES" ]
then
   LOGGER I "GUNZIP_TARGET was set to YES in the INI"
else   
   LOGGER I "GUNZIP_TARGET was either ommitted or not set correctly in the INI"
   LOGGER I "Setting GUNZIP_TARGET to NO"
   GUNZIP_TARGET="NO"
fi


# The absence of an Spark INIT file will not be a critical failure
# This is an optional parameter
CHECK_NULL_VAR SPARK_INIT_FILE "${SPARK_INIT_FILE}"
if ! [ "X${SPARK_INIT_FILE}" = "X" ]
then
   # However, if the parameter is populated, it must done so
   # with a valid INIT file
   CHECK_FILE_EXISTS SPARK_INIT_FILE "${SPARK_INIT_FILE}" 125
fi


# Initilaizations completed
# Now here is where we'll actually begin the processing

# create a LOCK file
#LOGGER I "Begin - Creating LOCK_FILE ${LOCK_FILE}"
#touch ${LOCK_FILE}
#if [ ${?} -ne 0 ]
#then
#   LOGGER E "********** ERROR **********"
#   LOGGER E "could not create LOCK_FILE ${LOCK_FILE}"
#   FAILURE 117
#fi
#LOGGER I "End - Creating LOCK_FILE ${LOCK_FILE}"

#set the retry time (count) value from INI if not set it to default
RETRY_NUM=${FAILED_RETRY_TIME}
if  [ -n "${RETRY_NUM}" ]
then
   LOGGER I "The value for the RETRY COUNT is set to ${RETRY_NUM}"
else
   RETRY_NUM=3
   LOGGER I "The default value for the RETRY COUNT is set to ${RETRY_NUM}"
fi

SLEEP=${SLEEP_TIME}
if  [ -n "${SLEEP}" ]
then
   LOGGER I "The value for the SLEEP TIME is set to ${SLEEP}"
else
   SLEEP=0
   LOGGER I "The default value for the SLEEP TIME is set to ${SLEEP}"
fi

# Here is where we'll finally run SPARK SQL
# It is assumed that SPARK-SUBMIT can be found via PATH
COUNTER=0
   # run Spark-submit
   LOGGER I "Begin - Run Spark-Submit"
   while [ ${COUNTER} -lt ${RETRY_NUM} ]
    do
        if [ ${SPARK_MAJOR_VERSION} = "2" ]
        then
            LOGGER I "Begin - Run Spark2-Submit"
            ${SPARK2_SUBMIT} --verbose \
            --queue ${QUEUE} \
            --num-executors ${NO_OF_EXECUTERS} \
            --executor-memory ${EXECUTOR_MEMORY} \
            --executor-cores ${EXECUTOR_CORES} \
            --driver-memory ${DRIVER_MEMORY} \
            --class ${DRIVER_CLASS_NAME} \
            --jars ${JAR_FILES} \
            ${SPARK2_DRIVER_JAR} \
            appName=${APP_NAME} \
            sparkMaster=${SPARK_MASTER} \
            inifile=${INI_FILE} \
            job=${INI_SECTION} \
            job_id=${JOB_ID} \
            1>>${LOG_FILE}.tmp 2>&1
        else
            LOGGER I "Begin - Run Spark1-Submit"
            ${SPARK_SUBMIT} --verbose \
            --queue ${QUEUE} \
            --num-executors ${NO_OF_EXECUTERS} \
            --executor-memory ${EXECUTOR_MEMORY} \
            --executor-cores ${EXECUTOR_CORES} \
            --driver-memory ${DRIVER_MEMORY} \
            --class ${DRIVER_CLASS_NAME} \
            --jars ${JAR_FILES} \
            ${SPARK1_DRIVER_JAR} \
            appName=${APP_NAME} \
            sparkMaster=${SPARK_MASTER} \
            inifile=${INI_FILE} \
            job=${INI_SECTION} \
            job_id=${JOB_ID} \
            1>>${LOG_FILE}.tmp 2>&1
        fi
      SPARK_RETCODE=${?}
      LOGGER I "Spark Submit return code: ${SPARK_RETCODE}"
      if [ ${SPARK_RETCODE} -eq 0 ]
      then
         # this .tmp log is to ensure that we capture all of the Spark-Submit output to the log
         LOGGER I "`cat ${LOG_FILE}.tmp`i"
         rm -f ${LOG_FILE}.tmp
         LOGGER I "Running Spark-Submit is succesfully completed"
         break
      else
         LOGGER E "********** ERROR **********"
         LOGGER E "Something went wrong in running the Spark-Submit, exiting"
         LOGGER I "`cat ${LOG_FILE}.tmp`i"
         rm -f ${LOG_FILE}.tmp
         let COUNTER=${COUNTER}+1
         let ATTEMPT=${COUNTER}+1

      fi
      if [ ${COUNTER} -ge ${RETRY_NUM} ]
      then
         LOGGER E "********** ERROR **********"
                 LOGGER E " Max number of retries: ${RETRY_NUM} reached. Research the error, and correct the condition before restarting"
         LOGGER E "Spark-Submit exited with a non-zero exit code: ${SPARK_RET_CODE}"
         LOGGER E "Be cognizant of any data which may have already processed prior"
         LOGGER E "to the error (be careful not to rerun incorrectly"
         LOGGER E "and potentially duplicating data)"
         FAILURE 126
      fi
         LOGGER I "RETRY ATTEMPT :  ${ATTEMPT}"
         sleep ${SLEEP}

   done
   LOGGER I "End - Run Spark-Submit"



# Processing has completed successfully, remove the lock file
#LOGGER I "Begin - removing LOCK FILE ${LOCK_FILE}"
#rm -f ${LOCK_FILE} >/dev/null
#if [ ${?} -ne 0 ]
#then
#   LOGGER E "********** ERROR **********"
#   LOGGER E "could not remove LOCK_FILE ${LOCK_FILE}"
#   FAILURE 116
#fi
#LOGGER I "End - removing LOCK FILE ${LOCK_FILE}"


# clean up old log files from the log archive directory
LOGGER I "Begin - Removing ${JOB_NAME} log files from ${LOG_DIR}/archive older than ${LOG_RETAIN_DAYS} days"
for X in `find "${LOG_DIR}/archive" "${PROGNAMESHORT}_${JOB_NAME}*" -mtime +${LOG_RETAIN_DAYS}`; do
   LOGGER I "removing file: ${X}"
   rm -f ${X}
   if [ ${?} -ne 0 ]
   then
      LOGGER E "********** ERROR **********"
      LOGGER E "Error removing file ${X}"
      FAILURE 105
   fi
done
LOGGER I "End - Removing ${JOB_NAME} log files from ${LOG_DIR}/archive older than ${LOG_RETAIN_DAYS} days"   

LOGGER I "Close log file and move into tar: ${LOG_DIR}/archive/${PROGNAMESHORT}_${JOB_NAME}_`date +%Y%m%d`.tar"
LOGGER I "********** END PROCESS **********"

# Upon successful completion of this script, we'll move the log file to the archive directory
# Becuase we're finalizing the logging, the last cleanup step will be done without logging
# Because multiple threads of this process may multiple times per day,
# the logs in the archive directory will be maintained in a tar ball by date
cd ${LOG_DIR}
gzip `basename ${LOG_FILE}` 
tar -rf ${LOG_DIR}/archive/${PROGNAMESHORT}_${JOB_NAME}_`date +%Y%m%d`.tar `basename ${LOG_FILE}`.gz
if [ ${?} -eq 0 ]
then
   # Remove the untarred log file once it's been successfully added to the archive tar
   # if for some reason the tar command fails, the untarred log file will remain in LOG_DIR
   rm -f ${LOG_FILE}.gz
fi

exit 0

-----------------------
----spark+streaming_wrapper.sh--
#################################################################################
# v1.0 - 2014-01-03 - Initial creation
# CEDM - Spark Streaming wrapper to run the spark-submit with parameterization
#################################################################################
# This script is a wrapper for running Hive SQL files 
#
# Return Codes:
#    0   - sucessful execution
#    101 - a required command line input is either missing or invalid
#    102 - an unrecognized input switch was provided
#    103 - could not create temporary log file
#    104 - a required variable was not defined in the INI_FILE
#    105 - could not remove an old log file
#    106 - the Lock file (LOCK_FILE) is not writable
#    107 - the Lock file (LOCK_FILE) already exists.  A previous scheduled
#          processes is either still running or has failed mid-stream
#    112 - the Log Directory (LOG_DIR) does not exist
#    113 - the archive subdirectory of the Log Directory (LOG_DIR/archive) does not exist
#    114 - the Log Directory (LOG_DIR) is not writable
#    115 - the archive subdirectory of the Log Directory (LOG_DIR/archive) is not writable"
#    116 - could not remove LOCK_FILE after completion
#    117 - could not ceate LOCK_FILE after initializations
#    124 - INI file does not exist
#    125 - referenced SQL file does not exist
#    126 - SPARK returned a non-zero exit code
#
#################################################################################
# Define all the functions (in the future, potential to use fpath)
#set -x

function PROGRAM_USAGE(){
   echo
   echo "Usage:  ./spark_sql_wrapper.sh -i [INI_FILE] -s [INI_SECTION] -r [RUNTIME_INI]"
   echo "   -i is required.  This is the fully qualified (pathed) INI filename"
   echo "   -s is required.  This is the section in the INI file pertaining to"
   echo "                    the specific execution.  Values are case specific"
   echo "   -r is required.  This is the fully qualified Runtime INI filename(Job Specific)"
   echo
   echo "Example:"
   echo "   /full/path/spark_sql_wrapper.sh -i /full/path/something.ini -s SECTION -r /full/path/runtime.ini"
   echo
   echo "Return Codes:"
   echo "   0   - sucessful execution"
   echo "   101 - a required command line input is either missing or invalid"
   echo "   102 - an unrecognized input switch was provided"
   echo "   103 - could not create temporary log file"
   echo "   104 - a required variable was not defined in the INI_FILE"
   echo "   105 - could not remove an old log file"
   echo "   106 - the Lock file (LOCK_FILE) is not writable"
   echo "   107 - the Lock file (LOCK_FILE) already exists.  A previous scheduled"
   echo "         process is either still running or has failed mid-stream"
   echo "   112 - the Log Directory (LOG_DIR) does not exist"
   echo "   113 - the archive subdirectory of the Log Directory (LOG_DIR/archive) does not exist"
   echo "   114 - the Log Directory (LOG_DIR) is not writable"
   echo "   115 - the archive subdirectory of the Log Directory (LOG_DIR/archive) is not writable"
   echo "   116 - could not remove LOCK_FILE after SFTP completion"
   echo "   117 - could not ceate LOCK_FILE after initializations"
   echo "   124 - INI file does not exist"
   echo "   125 - referenced SQL file does not exist"
   echo "   126 - SPARK returned a non-zero exit code"
   echo
}   

function PARSE_INI(){
   # function inputs:
   # ${1} = fully qualified path to INI file
   # ${2} = name of the section

   eval `sed -e 's/[[:space:]]*\=[[:space:]]*/=/g' \
       -e 's/;.*$//' \
       -e 's/[[:space:]]*$//' \
       -e 's/^[[:space:]]*//' \
       -e "s/^\(.*\)=\([^\"']*\)$/\1=\"\2\"/" \
      < ${1} \
       | sed -n -e "/^\[${2}\]/,/^\s*\[/{/^[^;].*\=.*/p;}"`
}

function LOGGER(){
   # function inputs:
   # ${1} = message type (I, W, E)
   # ${2} = message to be logged
   #
   # The fields in the log message are dash (-) delimited, and are formatted as::
   #   field 1, srting(29) - timestamp in format:  YYYY-MM-DD HH:MI:SS.NNNNNNNNN 
   #   field 2, string(7), right padded with spaces - message type (INFO, WARNING, ERROR), 
   #   field 3, string(n) - descriptive message

   # default message type is Info
   MESSAGE_TYPE="INFO   "  

   if [ "${1}" = "W" ]
   then 
      MESSAGE_TYPE="WARNING"
   elif [ "${1}" = "E" ]
   then
     MESSAGE_TYPE="ERROR  "
   fi 

   echo `date "+%Y-%m-%d %H:%M:%S.%N"`-"${MESSAGE_TYPE}"-"${2}" | tee -a ${LOG_FILE}
}

function FAILURE(){
   # funtion inputs:
   # ${1} = the return code to exit with

   LOGGER E "Exiting with custom return code ${1}"

#mailx -s "${JOB_NAME} Failure" ${EMAIL} << EOF
#The ${JOB_NAME} Job has failed.  Please review the log file located at ${HOSTNAME}:${LOG_FILE} for more info.
#Here is the log generated during this run.

#`cat ${LOG_FILE}`
#EOF
   
   exit ${1}
}

function CHECK_NULL_VAR(){
   # function inputs:
   # ${1} = variable name
   # ${2} = variable value
   # ${3} = exit code to use when check fails
   #        if exit code is null, then do not fail

   LOGGER I "Begin - confirm ${1} has been set and has a value"
   if [ "X${2}" = "X" ]
   then
      if [ "X${3}" = "X" ]
      then
         LOGGER I "Value for ${1} is null (not an error condition)"
      else
         LOGGER E "********** ERROR **********"
         LOGGER E "A valid value for ${1} has not been set"
         FAILURE ${3};
      fi
   fi
   LOGGER I "${1} has been set to: ${2}"
   LOGGER I "End - confirm ${1} has been set and has a value"
}

function CHECK_FILE_WRITABLE(){
   # function inputs
   # ${1} = variable name
   # ${2} = variable value - fully qualified file name
   # ${3} = exit code to use when check fails
   #        if exit code is null, then do not fail

   # need a variable so we know if we need to RM our test file or not
   TEMP_TOUCH_FLAG=0

   LOGGER I "Begin - Check if ${1} file ${2} is writable"
   # first we have to check if the exists or not
   if ! [ -f ${2} ]
   then
      LOGGER I "Touching a temp file for writable validation"
      TEMP_TOUCH_FLAG=1
      touch ${2} > /dev/null 2>&1
   fi

   if ! [ -w ${2} ]
   then
      if [ "X${3}" = "X" ]
      then
         LOGGER W "${1} file ${2} is not writable (not an error condition)"
      else
         LOGGER E "********* ERROR **********"
         LOGGER E "${1} file ${2} is not wiritable"
         LOGGER E "Check the location and permissions"
         FAILURE ${3}
      fi
   fi

   if [ ${TEMP_TOUCH_FLAG} -ne 0 ]
   then
      LOGGER I "Removing the temp file used for writable validation"
      rm -f ${2} > /dev/null 2>&1
   fi

   LOGGER I "End - check if ${1} file ${2} is writable"
}

function CHECK_FILE_EXISTS(){
   # function inputs
   # ${1} = variable name
   # ${2} = varibale value - fully qualified file name
   # ${3} = exit code to use when check fails
   #        if exit code is null, then do not fail

   LOGGER I "Begin - Check if ${1} file ${2} exists"
   if ! [ -f ${2} ]
   then
      if [ "X${3}" = "X" ]
      then
         LOGGER W "${1} file ${2} does not exist (not an error condition)"
      else
         LOGGER E "********* ERROR **********"
         LOGGER E "${1} file ${2} does not exist"
         LOGGER E "Research cause of missing file, create or restore file if needed"
         FAILURE ${3}
      fi
   else
      LOGGER I "${1} file ${2} exists"
   fi
   LOGGER I "End - check if ${1} file ${2} exists"
}

# Done defining the functions
#################################################################################
# Start the logical processing

# Set some internal variables
PROGNAMELONG=`basename $0`                                      # name of the program, minus all of the path info
PROGNAMESHORT=`echo ${PROGNAMELONG} | sed 's/\.[^.]*$//'`       # name of the program, minus the extension
DATETIME=`date +%Y%m%d%H%M%S`                                   # datetime stamp yyyymmddhhmiss
CURPID=$$                              # PID of the current running process (knowing this helps with concurrency)


# Before we can initialize the dynamic log management, we have to parse the
# command line and INI to set all of the varibale options.  However, we do want
# to generate a log right immediately so that we can log the process of parsing
# the command line and INI (to be able to troubleshoot if those steps fail).
# We'll start by creating a temporary log in the current working directory.
# 
# Dynamic log management will be initialized only after a successful read of the 
# command line and INI options.  Once that is ready, we'll move the log out of
# current working directory and into the directory structrue defined in the INI.

TEMP_LOG_DIR=/tmp                                              # just the current working directory
TEMP_LOG_FILE=${TEMP_LOG_DIR}/${PROGNAMESHORT}_${DATETIME}_${CURPID}.log  # temporary log   
LOG_FILE=${TEMP_LOG_FILE}


# Now make sure that we can actually open and write the temporary log 
touch ${LOG_FILE} > /dev/null 2>&1
if ! [ -w ${LOG_FILE} ]; then
   echo "********** ERROR **********"
   echo "The temporary log file ${LOG_FILE} is not writeable"
   echo "Check the location and permissions and try again"
   FAILURE 103
fi

# OK, now we can start actually logging
LOGGER I "********** BEGIN PROCESS **********"
LOGGER I "Full command line: ${0} ${*}"
LOGGER I "The current unix pid is: ${CURPID}"

# Parse the command line options provided
LOGGER I "Begin - parse command line options with getopts"
while getopts :i:s:r: OPTIONS
do
   case ${OPTIONS} in
      i)  INI_FILE=${OPTARG};;
      s)  INI_SECTION=${OPTARG};;
      r)  RUNTIME_INI=${OPTARG};;
     \?)  LOGGER E "********** ERROR **********"
          LOGGER E "A required switch was missing, or an invalid switch was supplied"
          PROGRAM_USAGE
          FAILURE 102;; 
   esac
done
LOGGER I "End - parse command line options with getopts"


# check to ensure that the required -i switch for INI_FILE was provided
CHECK_NULL_VAR INI_FILE "${INI_FILE}" 101
# Confirm that the INI file specified actualy exists
CHECK_FILE_EXISTS INI_FILE "${INI_FILE}" 124

# check to ensure that the required -i switch for RUNTIME_INI was provided
CHECK_NULL_VAR RUNTIME_INI "${RUNTIME_INI}" 101
# Confirm that the INI file specified actualy exists
CHECK_FILE_EXISTS RUNTIME_INI "${RUNTIME_INI}" 124

# check to ensure that the required -s switch for INI_SECTION was provided
CHECK_NULL_VAR INI_SECTION "${INI_SECTION}" 101

# Source global parameters from the INI file
LOGGER I "Begin - read GLOBAL varibales from INI_FILE"
PARSE_INI ${INI_FILE} GLOBAL
LOGGER I "End - read GLOBAL varibales from INI_FILE"

# Source global parameters from the Runtime INI file
LOGGER I "Begin - read GLOBAL varibales from RUNTIME_INI"
PARSE_INI ${RUNTIME_INI} GLOBAL
LOGGER I "End - read GLOBAL varibales from RUNTIME_INI"

# source process-specific parameters from the INI file
LOGGER I "Begin - read process-specifc variables from INI_FILE for INI_SECTION"
PARSE_INI ${INI_FILE} ${INI_SECTION}
LOGGER I "End - read process-specific variables from INI_FILE for INI_SECTION"

# OK, now we need to validate that other expected variables have been set by the INI
# process, and that access to additional files or directories are as expected
CHECK_NULL_VAR EMAIL "${EMAIL}" 104
CHECK_NULL_VAR EXECUTION_MODE "${EXECUTION_MODE}" 104
CHECK_NULL_VAR SPARK_MAJOR_VERSION "${SPARK_MAJOR_VERSION}" 104
CHECK_NULL_VAR SPARK2_SUBMIT "${SPARK2_SUBMIT}" 104
CHECK_NULL_VAR SPARK_SUBMIT "${SPARK_SUBMIT}" 104
CHECK_NULL_VAR QUEUE "${QUEUE}" 104
CHECK_NULL_VAR NO_OF_EXECUTERS "${NO_OF_EXECUTERS}" 104
CHECK_NULL_VAR EXECUTOR_MEMORY "${EXECUTOR_MEMORY}" 104
CHECK_NULL_VAR EXECUTOR_CORES "${EXECUTOR_CORES}" 104
CHECK_NULL_VAR DRIVER_MEMORY "${DRIVER_MEMORY}" 104
CHECK_NULL_VAR JAAS_FILE "${JAAS_FILE}" 104
CHECK_NULL_VAR JAAS_FILE_NAME "${JAAS_FILE_NAME}" 104
CHECK_NULL_VAR KEYTAB_FILE "${KEYTAB_FILE}" 104
CHECK_NULL_VAR SPARK2_DRIVER_JAR "${SPARK2_DRIVER_JAR}" 104
CHECK_NULL_VAR SPARK1_DRIVER_JAR "${SPARK1_DRIVER_JAR}" 104
CHECK_NULL_VAR DRIVER_CLASS_NAME "${DRIVER_CLASS_NAME}" 104
CHECK_NULL_VAR JAR_FILES "${JAR_FILES}" 104

CHECK_NULL_VAR LOG_DIR "${LOG_DIR}" 104

# Check to ensure the log directory exists
# note - we could also decide to attempt to create it within the script
# but my fear is that if the directory disappears, that there might be a
# larger issue occurring
if ! [ -d ${LOG_DIR} ]
then
   LOGGER E "********** ERROR **********"
   LOGGER E "The LOG_DIR ${LOG_DIR} does not exist"
   LOGGER E "Ensure that a valid directory with the proper access exists"
   FAILURE 112
fi

# check to confirm that the log directory is writable
CHECK_FILE_WRITABLE LOG_DIR_WRITABLE_CHECK ${LOG_DIR}/temp_${CURPID}.log 114

# Check to ensure the archive subdirectory of the log directory exists
if ! [ -d ${LOG_DIR}/archive ]
then
   LOGGER E "********** ERROR **********"
   LOGGER E "The archive subdirectory of LOG_DIR ${LOG_DIR} does not exist"
   LOGGER E "Ensure that a valid directory with the proper access exists"
   FAILURE 113
fi

# check to ensure that the archive subdirectory of the log directory is writable
CHECK_FILE_WRITABLE LOG_DIR_ARCHIVE_WRITABLE_CHECK ${LOG_DIR}/archive/temp_${CURPID}.log 115

CHECK_NULL_VAR JOB_NAME "${JOB_NAME}" 104

CHECK_NULL_VAR JOB_ID "${JOB_ID}" 104

#CHECK_NULL_VAR LOCK_FILE "${LOCK_FILE}" 104
# Intentionally not using the CHECK_FILE_EXIST function here
# In this case, I want the process to fail if the file does
# exist instead of failing when it does not
#if [ -f ${LOCK_FILE} ]
#then
#   LOGGER E "********** ERROR **********"
#   LOGGER E "The LOCK_FILE ${LOCK_FILE} already exists"
#   LOGGER E "This indicates that either the previously scheduled run of"
#   LOGGER E "the job is still running, or that the previous run of the job"
#   LOGGER E "has failed.  The root cause needs to be researched before"
#   LOGGER E "the process can be allowed to run, else you run the risk of"
#   LOGGER E "messing up the High Water Mark and perhaps missing files."
#   FAILURE 107
#fi
#CHECK_FILE_WRITABLE LOCK_FILE "${LOCK_FILE}" 106

# OK, now at this point all varaibles and parameters have been set
# so we'll switch the logging over to the correct log store.

LOGGER I "Begin - switching temporary log file ${TEMP_LOG_FILE} to managed structure"
LOG_FILE=${LOG_DIR}/${PROGNAMESHORT}_${JOB_NAME}_${DATETIME}_${CURPID}.log
mv -f ${TEMP_LOG_FILE} ${LOG_FILE} 
LOGGER I "The LOG_FILE is now being written to ${LOG_FILE}"
LOGGER I "End - switching temporary LOG to managed structure"

# check optional varibale GUNZIP_TARGET
if [ "${GUNZIP_TARGET}" = "YES" ]
then
   LOGGER I "GUNZIP_TARGET was set to YES in the INI"
else   
   LOGGER I "GUNZIP_TARGET was either ommitted or not set correctly in the INI"
   LOGGER I "Setting GUNZIP_TARGET to NO"
   GUNZIP_TARGET="NO"
fi


# The absence of an Spark INIT file will not be a critical failure
# This is an optional parameter
CHECK_NULL_VAR SPARK_INIT_FILE "${SPARK_INIT_FILE}"
if ! [ "X${SPARK_INIT_FILE}" = "X" ]
then
   # However, if the parameter is populated, it must done so
   # with a valid INIT file
   CHECK_FILE_EXISTS SPARK_INIT_FILE "${SPARK_INIT_FILE}" 125
fi


# Initilaizations completed
# Now here is where we'll actually begin the processing

# create a LOCK file
#LOGGER I "Begin - Creating LOCK_FILE ${LOCK_FILE}"
#touch ${LOCK_FILE}
#if [ ${?} -ne 0 ]
#then
#   LOGGER E "********** ERROR **********"
#   LOGGER E "could not create LOCK_FILE ${LOCK_FILE}"
#   FAILURE 117
#fi
#LOGGER I "End - Creating LOCK_FILE ${LOCK_FILE}"

#set the retry time (count) value from INI if not set it to default
RETRY_NUM=${FAILED_RETRY_TIME}
if  [ -n "${RETRY_NUM}" ]
then
   LOGGER I "The value for the RETRY COUNT is set to ${RETRY_NUM}"
else
   RETRY_NUM=3
   LOGGER I "The default value for the RETRY COUNT is set to ${RETRY_NUM}"
fi

SLEEP=${SLEEP_TIME}
if  [ -n "${SLEEP}" ]
then
   LOGGER I "The value for the SLEEP TIME is set to ${SLEEP}"
else
   SLEEP=0
   LOGGER I "The default value for the SLEEP TIME is set to ${SLEEP}"
fi

# Here is where we'll finally run SPARK SQL
# It is assumed that SPARK-SUBMIT can be found via PATH
COUNTER=0
   # run Spark-submit
   LOGGER I "Begin - Run Spark-Submit"
   while [ ${COUNTER} -lt ${RETRY_NUM} ]
    do
        if [ ${SPARK_MAJOR_VERSION} = "2" ]
        then
            LOGGER I "Begin - Run Spark2-Submit"
            ${SPARK2_SUBMIT} --verbose \
            --master ${SPARK_MASTER} \
            --deploy-mode ${SPARK_DEPLOY_MODE} \
            --queue ${QUEUE} \
            --num-executors ${NO_OF_EXECUTERS} \
            --executor-memory ${EXECUTOR_MEMORY} \
            --executor-cores ${EXECUTOR_CORES} \
            --driver-memory ${DRIVER_MEMORY} \
            --files "${JAAS_FILE},${KEYTAB_FILE}" \
            --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=${JAAS_FILE_NAME}" \
            --driver-java-options "-Djava.security.auth.login.config=${JAAS_FILE}" \
            --class ${DRIVER_CLASS_NAME} \
            --jars ${JAR_FILES} \
            ${SPARK2_DRIVER_JAR} \
            appName=${APP_NAME} \
            sparkMaster=${SPARK_MASTER} \
            inifile=${INI_FILE} \
            job=${INI_SECTION} \
            job_id=${JOB_ID} \
            1>>${LOG_FILE}.tmp 2>&1
        else
            LOGGER I "Begin - Run Spark1-Submit"
            ${SPARK_SUBMIT} --verbose \
            --master ${SPARK_MASTER} \
            --deploy-mode ${SPARK_DEPLOY_MODE} \
            --queue ${QUEUE} \
            --num-executors ${NO_OF_EXECUTERS} \
            --executor-memory ${EXECUTOR_MEMORY} \
            --executor-cores ${EXECUTOR_CORES} \
            --driver-memory ${DRIVER_MEMORY} \
            --files "${JAAS_FILE},${KEYTAB_FILE}" \
            --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=${JAAS_FILE_NAME}" \
            --driver-java-options "-Djava.security.auth.login.config=${JAAS_FILE}" \
            --class ${DRIVER_CLASS_NAME} \
            --jars ${JAR_FILES} \
            ${SPARK1_DRIVER_JAR} \
            appName=${APP_NAME} \
            sparkMaster=${SPARK_MASTER} \
            inifile=${INI_FILE} \
            job=${INI_SECTION} \
            job_id=${JOB_ID} \
            1>>${LOG_FILE}.tmp 2>&1
        fi
      SPARK_RETCODE=${?}
      LOGGER I "Spark Submit return code: ${SPARK_RETCODE}"
      if [ ${SPARK_RETCODE} -eq 0 ]
      then
         # this .tmp log is to ensure that we capture all of the Spark-Submit output to the log
         LOGGER I "`cat ${LOG_FILE}.tmp`i"
         rm -f ${LOG_FILE}.tmp
         LOGGER I "Running Spark-Submit is succesfully completed"
         break
      else
         LOGGER E "********** ERROR **********"
         LOGGER E "Something went wrong in running the Spark-Submit, exiting"
         LOGGER I "`cat ${LOG_FILE}.tmp`i"
         rm -f ${LOG_FILE}.tmp
         let COUNTER=${COUNTER}+1
         let ATTEMPT=${COUNTER}+1

      fi
      if [ ${COUNTER} -ge ${RETRY_NUM} ]
      then
         LOGGER E "********** ERROR **********"
                 LOGGER E " Max number of retries: ${RETRY_NUM} reached. Research the error, and correct the condition before restarting"
         LOGGER E "Spark-Submit exited with a non-zero exit code: ${SPARK_RET_CODE}"
         LOGGER E "Be cognizant of any data which may have already processed prior"
         LOGGER E "to the error (be careful not to rerun incorrectly"
         LOGGER E "and potentially duplicating data)"
         FAILURE 126
      fi
         LOGGER I "RETRY ATTEMPT :  ${ATTEMPT}"
         sleep ${SLEEP}

   done
   LOGGER I "End - Run Spark-Submit"



# Processing has completed successfully, remove the lock file
#LOGGER I "Begin - removing LOCK FILE ${LOCK_FILE}"
#rm -f ${LOCK_FILE} >/dev/null
#if [ ${?} -ne 0 ]
#then
#   LOGGER E "********** ERROR **********"
#   LOGGER E "could not remove LOCK_FILE ${LOCK_FILE}"
#   FAILURE 116
#fi
#LOGGER I "End - removing LOCK FILE ${LOCK_FILE}"


# clean up old log files from the log archive directory
LOGGER I "Begin - Removing ${JOB_NAME} log files from ${LOG_DIR}/archive older than ${LOG_RETAIN_DAYS} days"
for X in `find "${LOG_DIR}/archive" "${PROGNAMESHORT}_${JOB_NAME}*" -mtime +${LOG_RETAIN_DAYS}`; do
   LOGGER I "removing file: ${X}"
   rm -f ${X}
   if [ ${?} -ne 0 ]
   then
      LOGGER E "********** ERROR **********"
      LOGGER E "Error removing file ${X}"
      FAILURE 105
   fi
done
LOGGER I "End - Removing ${JOB_NAME} log files from ${LOG_DIR}/archive older than ${LOG_RETAIN_DAYS} days"   

LOGGER I "Close log file and move into tar: ${LOG_DIR}/archive/${PROGNAMESHORT}_${JOB_NAME}_`date +%Y%m%d`.tar"
LOGGER I "********** END PROCESS **********"

# Upon successful completion of this script, we'll move the log file to the archive directory
# Becuase we're finalizing the logging, the last cleanup step will be done without logging
# Because multiple threads of this process may multiple times per day,
# the logs in the archive directory will be maintained in a tar ball by date
cd ${LOG_DIR}
gzip `basename ${LOG_FILE}` 
tar -rf ${LOG_DIR}/archive/${PROGNAMESHORT}_${JOB_NAME}_`date +%Y%m%d`.tar `basename ${LOG_FILE}`.gz
if [ ${?} -eq 0 ]
then
   # Remove the untarred log file once it's been successfully added to the archive tar
   # if for some reason the tar command fails, the untarred log file will remain in LOG_DIR
   rm -f ${LOG_FILE}.gz
fi

exit 0

--------------
------ Spark_submit_wrapper.sh---
#################################################################################
# v1.0 - 2014-01-03 - Initial creation
# CEDM - spark-submit wrapper to run Spark Batch and Streaming Jobs with parameterization
#################################################################################
# This script is a wrapper for running Hive SQL files 
#
# Return Codes:
#    0   - sucessful execution
#    101 - a required command line input is either missing or invalid
#    102 - an unrecognized input switch was provided
#    103 - could not create temporary log file
#    104 - a required variable was not defined in the INI_FILE
#    105 - could not remove an old log file
#    106 - the Lock file (LOCK_FILE) is not writable
#    107 - the Lock file (LOCK_FILE) already exists.  A previous scheduled
#          processes is either still running or has failed mid-stream
#    112 - the Log Directory (LOG_DIR) does not exist
#    113 - the archive subdirectory of the Log Directory (LOG_DIR/archive) does not exist
#    114 - the Log Directory (LOG_DIR) is not writable
#    115 - the archive subdirectory of the Log Directory (LOG_DIR/archive) is not writable"
#    116 - could not remove LOCK_FILE after completion
#    117 - could not ceate LOCK_FILE after initializations
#    124 - INI file does not exist
#    125 - referenced SQL file does not exist
#    126 - SPARK returned a non-zero exit code
#
#################################################################################
# Define all the functions (in the future, potential to use fpath)
#set -x

function PROGRAM_USAGE(){
   echo
   echo "Usage:  ./spark_sql_wrapper.sh -i [INI_FILE] -s [INI_SECTION] -r [RUNTIME_INI]"
   echo "   -i is required.  This is the fully qualified (pathed) INI filename"
   echo "   -s is required.  This is the section in the INI file pertaining to"
   echo "                    the specific execution.  Values are case specific"
   echo "   -r is required.  This is the fully qualified Runtime INI filename(Job Specific)"
   echo
   echo "Example:"
   echo "   /full/path/spark_sql_wrapper.sh -i /full/path/something.ini -s SECTION -r /full/path/runtime.ini"
   echo
   echo "Return Codes:"
   echo "   0   - sucessful execution"
   echo "   101 - a required command line input is either missing or invalid"
   echo "   102 - an unrecognized input switch was provided"
   echo "   103 - could not create temporary log file"
   echo "   104 - a required variable was not defined in the INI_FILE"
   echo "   105 - could not remove an old log file"
   echo "   106 - the Lock file (LOCK_FILE) is not writable"
   echo "   107 - the Lock file (LOCK_FILE) already exists.  A previous scheduled"
   echo "         process is either still running or has failed mid-stream"
   echo "   112 - the Log Directory (LOG_DIR) does not exist"
   echo "   113 - the archive subdirectory of the Log Directory (LOG_DIR/archive) does not exist"
   echo "   114 - the Log Directory (LOG_DIR) is not writable"
   echo "   115 - the archive subdirectory of the Log Directory (LOG_DIR/archive) is not writable"
   echo "   116 - could not remove LOCK_FILE after SFTP completion"
   echo "   117 - could not ceate LOCK_FILE after initializations"
   echo "   124 - INI file does not exist"
   echo "   125 - referenced SQL file does not exist"
   echo "   126 - SPARK returned a non-zero exit code"
   echo
}   

function PARSE_INI(){
   # function inputs:
   # ${1} = fully qualified path to INI file
   # ${2} = name of the section

   eval `sed -e 's/[[:space:]]*\=[[:space:]]*/=/g' \
       -e 's/;.*$//' \
       -e 's/[[:space:]]*$//' \
       -e 's/^[[:space:]]*//' \
       -e "s/^\(.*\)=\([^\"']*\)$/\1=\"\2\"/" \
      < ${1} \
       | sed -n -e "/^\[${2}\]/,/^\s*\[/{/^[^;].*\=.*/p;}"`
}

function LOGGER(){
   # function inputs:
   # ${1} = message type (I, W, E)
   # ${2} = message to be logged
   #
   # The fields in the log message are dash (-) delimited, and are formatted as::
   #   field 1, srting(29) - timestamp in format:  YYYY-MM-DD HH:MI:SS.NNNNNNNNN 
   #   field 2, string(7), right padded with spaces - message type (INFO, WARNING, ERROR), 
   #   field 3, string(n) - descriptive message

   # default message type is Info
   MESSAGE_TYPE="INFO   "  

   if [ "${1}" = "W" ]
   then 
      MESSAGE_TYPE="WARNING"
   elif [ "${1}" = "E" ]
   then
     MESSAGE_TYPE="ERROR  "
   fi 

   echo `date "+%Y-%m-%d %H:%M:%S.%N"`-"${MESSAGE_TYPE}"-"${2}" | tee -a ${LOG_FILE}
}

function FAILURE(){
   # funtion inputs:
   # ${1} = the return code to exit with

   LOGGER E "Exiting with custom return code ${1}"

#mailx -s "${JOB_NAME} Failure" ${EMAIL} << EOF
#The ${JOB_NAME} Job has failed.  Please review the log file located at ${HOSTNAME}:${LOG_FILE} for more info.
#Here is the log generated during this run.

#`cat ${LOG_FILE}`
#EOF
   
   exit ${1}
}

function CHECK_NULL_VAR(){
   # function inputs:
   # ${1} = variable name
   # ${2} = variable value
   # ${3} = exit code to use when check fails
   #        if exit code is null, then do not fail

   LOGGER I "Begin - confirm ${1} has been set and has a value"
   if [ "X${2}" = "X" ]
   then
      if [ "X${3}" = "X" ]
      then
         LOGGER I "Value for ${1} is null (not an error condition)"
      else
         LOGGER E "********** ERROR **********"
         LOGGER E "A valid value for ${1} has not been set"
         FAILURE ${3};
      fi
   fi
   LOGGER I "${1} has been set to: ${2}"
   LOGGER I "End - confirm ${1} has been set and has a value"
}

function CHECK_FILE_WRITABLE(){
   # function inputs
   # ${1} = variable name
   # ${2} = variable value - fully qualified file name
   # ${3} = exit code to use when check fails
   #        if exit code is null, then do not fail

   # need a variable so we know if we need to RM our test file or not
   TEMP_TOUCH_FLAG=0

   LOGGER I "Begin - Check if ${1} file ${2} is writable"
   # first we have to check if the exists or not
   if ! [ -f ${2} ]
   then
      LOGGER I "Touching a temp file for writable validation"
      TEMP_TOUCH_FLAG=1
      touch ${2} > /dev/null 2>&1
   fi

   if ! [ -w ${2} ]
   then
      if [ "X${3}" = "X" ]
      then
         LOGGER W "${1} file ${2} is not writable (not an error condition)"
      else
         LOGGER E "********* ERROR **********"
         LOGGER E "${1} file ${2} is not wiritable"
         LOGGER E "Check the location and permissions"
         FAILURE ${3}
      fi
   fi

   if [ ${TEMP_TOUCH_FLAG} -ne 0 ]
   then
      LOGGER I "Removing the temp file used for writable validation"
      rm -f ${2} > /dev/null 2>&1
   fi

   LOGGER I "End - check if ${1} file ${2} is writable"
}

function CHECK_FILE_EXISTS(){
   # function inputs
   # ${1} = variable name
   # ${2} = varibale value - fully qualified file name
   # ${3} = exit code to use when check fails
   #        if exit code is null, then do not fail

   LOGGER I "Begin - Check if ${1} file ${2} exists"
   if ! [ -f ${2} ]
   then
      if [ "X${3}" = "X" ]
      then
         LOGGER W "${1} file ${2} does not exist (not an error condition)"
      else
         LOGGER E "********* ERROR **********"
         LOGGER E "${1} file ${2} does not exist"
         LOGGER E "Research cause of missing file, create or restore file if needed"
         FAILURE ${3}
      fi
   else
      LOGGER I "${1} file ${2} exists"
   fi
   LOGGER I "End - check if ${1} file ${2} exists"
}

# Done defining the functions
#################################################################################
# Start the logical processing

# Set some internal variables
PROGNAMELONG=`basename $0`                                      # name of the program, minus all of the path info
PROGNAMESHORT=`echo ${PROGNAMELONG} | sed 's/\.[^.]*$//'`       # name of the program, minus the extension
DATETIME=`date +%Y%m%d%H%M%S`                                   # datetime stamp yyyymmddhhmiss
CURPID=$$                              # PID of the current running process (knowing this helps with concurrency)


# Before we can initialize the dynamic log management, we have to parse the
# command line and INI to set all of the varibale options.  However, we do want
# to generate a log right immediately so that we can log the process of parsing
# the command line and INI (to be able to troubleshoot if those steps fail).
# We'll start by creating a temporary log in the current working directory.
# 
# Dynamic log management will be initialized only after a successful read of the 
# command line and INI options.  Once that is ready, we'll move the log out of
# current working directory and into the directory structrue defined in the INI.

TEMP_LOG_DIR=/tmp                                              # just the current working directory
TEMP_LOG_FILE=${TEMP_LOG_DIR}/${PROGNAMESHORT}_${DATETIME}_${CURPID}.log  # temporary log   
LOG_FILE=${TEMP_LOG_FILE}


# Now make sure that we can actually open and write the temporary log 
touch ${LOG_FILE} > /dev/null 2>&1
if ! [ -w ${LOG_FILE} ]; then
   echo "********** ERROR **********"
   echo "The temporary log file ${LOG_FILE} is not writeable"
   echo "Check the location and permissions and try again"
   FAILURE 103
fi

# OK, now we can start actually logging
LOGGER I "********** BEGIN PROCESS **********"
LOGGER I "Full command line: ${0} ${*}"
LOGGER I "The current unix pid is: ${CURPID}"

# Parse the command line options provided
LOGGER I "Begin - parse command line options with getopts"
while getopts :i:s:r: OPTIONS
do
   case ${OPTIONS} in
      i)  INI_FILE=${OPTARG};;
      s)  INI_SECTION=${OPTARG};;
      #r)  RUNTIME_INI=${OPTARG};;
     \?)  LOGGER E "********** ERROR **********"
          LOGGER E "A required switch was missing, or an invalid switch was supplied"
          PROGRAM_USAGE
          FAILURE 102;; 
   esac
done
LOGGER I "End - parse command line options with getopts"


# check to ensure that the required -i switch for INI_FILE was provided
CHECK_NULL_VAR INI_FILE "${INI_FILE}" 101
# Confirm that the INI file specified actualy exists
CHECK_FILE_EXISTS INI_FILE "${INI_FILE}" 124

# check to ensure that the required -i switch for RUNTIME_INI was provided
#CHECK_NULL_VAR RUNTIME_INI "${RUNTIME_INI}" 101
# Confirm that the INI file specified actualy exists
#CHECK_FILE_EXISTS RUNTIME_INI "${RUNTIME_INI}" 124

# check to ensure that the required -s switch for INI_SECTION was provided
CHECK_NULL_VAR INI_SECTION "${INI_SECTION}" 101

# Source global parameters from the INI file
LOGGER I "Begin - read GLOBAL varibales from INI_FILE"
PARSE_INI ${INI_FILE} GLOBAL
LOGGER I "End - read GLOBAL varibales from INI_FILE"

# Source global parameters from the Runtime INI file
#LOGGER I "Begin - read GLOBAL varibales from RUNTIME_INI"
#PARSE_INI ${RUNTIME_INI} GLOBAL
#LOGGER I "End - read GLOBAL varibales from RUNTIME_INI"

# source process-specific parameters from the INI file
LOGGER I "Begin - read process-specifc variables from INI_FILE for INI_SECTION"
PARSE_INI ${INI_FILE} ${INI_SECTION}
LOGGER I "End - read process-specific variables from INI_FILE for INI_SECTION"

# OK, now we need to validate that other expected variables have been set by the INI
# process, and that access to additional files or directories are as expected
CHECK_NULL_VAR JOB_NAME "${JOB_NAME}" 104
CHECK_NULL_VAR EMAIL "${EMAIL}" 104
CHECK_NULL_VAR EXECUTION_MODE "${EXECUTION_MODE}" 104
CHECK_NULL_VAR SPARK_MAJOR_VERSION "${SPARK_MAJOR_VERSION}" 104
CHECK_NULL_VAR SPARK2_SUBMIT "${SPARK2_SUBMIT}" 104
CHECK_NULL_VAR SPARK_SUBMIT "${SPARK_SUBMIT}" 104
CHECK_NULL_VAR QUEUE "${QUEUE}" 104
CHECK_NULL_VAR NO_OF_EXECUTERS "${NO_OF_EXECUTERS}" 104
CHECK_NULL_VAR EXECUTOR_MEMORY "${EXECUTOR_MEMORY}" 104
CHECK_NULL_VAR EXECUTOR_CORES "${EXECUTOR_CORES}" 104
CHECK_NULL_VAR DRIVER_MEMORY "${DRIVER_MEMORY}" 104
CHECK_NULL_VAR JAAS_FILE "${JAAS_FILE}" 104
CHECK_NULL_VAR JAAS_FILE_NAME "${JAAS_FILE_NAME}" 104
CHECK_NULL_VAR KEYTAB_FILE "${KEYTAB_FILE}" 104
CHECK_NULL_VAR SPARK2_DRIVER_JAR "${SPARK2_DRIVER_JAR}" 104
CHECK_NULL_VAR SPARK1_DRIVER_JAR "${SPARK1_DRIVER_JAR}" 104
CHECK_NULL_VAR DRIVER_CLASS_NAME "${DRIVER_CLASS_NAME}" 104
CHECK_NULL_VAR JAR_FILES "${JAR_FILES}" 104
#CHECK_NULL_VAR JOB_ID "${JOB_ID}" 104

CHECK_NULL_VAR LOG_DIR "${LOG_DIR}" 104

# Check to ensure the log directory exists
# note - we could also decide to attempt to create it within the script
# but my fear is that if the directory disappears, that there might be a
# larger issue occurring
if ! [ -d ${LOG_DIR} ]
then
   LOGGER E "********** ERROR **********"
   LOGGER E "The LOG_DIR ${LOG_DIR} does not exist"
   LOGGER E "Ensure that a valid directory with the proper access exists"
   FAILURE 112
fi

# check to confirm that the log directory is writable
CHECK_FILE_WRITABLE LOG_DIR_WRITABLE_CHECK ${LOG_DIR}/temp_${CURPID}.log 114

# Check to ensure the archive subdirectory of the log directory exists
if ! [ -d ${LOG_DIR}/archive ]
then
   LOGGER E "********** ERROR **********"
   LOGGER E "The archive subdirectory of LOG_DIR ${LOG_DIR} does not exist"
   LOGGER E "Ensure that a valid directory with the proper access exists"
   FAILURE 113
fi

# check to ensure that the archive subdirectory of the log directory is writable
CHECK_FILE_WRITABLE LOG_DIR_ARCHIVE_WRITABLE_CHECK ${LOG_DIR}/archive/temp_${CURPID}.log 115

#CHECK_NULL_VAR LOCK_FILE "${LOCK_FILE}" 104
# Intentionally not using the CHECK_FILE_EXIST function here
# In this case, I want the process to fail if the file does
# exist instead of failing when it does not
#if [ -f ${LOCK_FILE} ]
#then
#   LOGGER E "********** ERROR **********"
#   LOGGER E "The LOCK_FILE ${LOCK_FILE} already exists"
#   LOGGER E "This indicates that either the previously scheduled run of"
#   LOGGER E "the job is still running, or that the previous run of the job"
#   LOGGER E "has failed.  The root cause needs to be researched before"
#   LOGGER E "the process can be allowed to run, else you run the risk of"
#   LOGGER E "messing up the High Water Mark and perhaps missing files."
#   FAILURE 107
#fi
#CHECK_FILE_WRITABLE LOCK_FILE "${LOCK_FILE}" 106

# OK, now at this point all varaibles and parameters have been set
# so we'll switch the logging over to the correct log store.

LOGGER I "Begin - switching temporary log file ${TEMP_LOG_FILE} to managed structure"
LOG_FILE=${LOG_DIR}/${PROGNAMESHORT}_${JOB_NAME}_${DATETIME}_${CURPID}.log
mv -f ${TEMP_LOG_FILE} ${LOG_FILE} 
LOGGER I "The LOG_FILE is now being written to ${LOG_FILE}"
LOGGER I "End - switching temporary LOG to managed structure"

# check optional varibale GUNZIP_TARGET
if [ "${GUNZIP_TARGET}" = "YES" ]
then
   LOGGER I "GUNZIP_TARGET was set to YES in the INI"
else   
   LOGGER I "GUNZIP_TARGET was either ommitted or not set correctly in the INI"
   LOGGER I "Setting GUNZIP_TARGET to NO"
   GUNZIP_TARGET="NO"
fi


# The absence of an Spark INIT file will not be a critical failure
# This is an optional parameter
CHECK_NULL_VAR SPARK_INIT_FILE "${SPARK_INIT_FILE}"
if ! [ "X${SPARK_INIT_FILE}" = "X" ]
then
   # However, if the parameter is populated, it must done so
   # with a valid INIT file
   CHECK_FILE_EXISTS SPARK_INIT_FILE "${SPARK_INIT_FILE}" 125
fi


# Initilaizations completed
# Now here is where we'll actually begin the processing

# create a LOCK file
#LOGGER I "Begin - Creating LOCK_FILE ${LOCK_FILE}"
#touch ${LOCK_FILE}
#if [ ${?} -ne 0 ]
#then
#   LOGGER E "********** ERROR **********"
#   LOGGER E "could not create LOCK_FILE ${LOCK_FILE}"
#   FAILURE 117
#fi
#LOGGER I "End - Creating LOCK_FILE ${LOCK_FILE}"

#set the retry time (count) value from INI if not set it to default
RETRY_NUM=${FAILED_RETRY_TIME}
if  [ -n "${RETRY_NUM}" ]
then
   LOGGER I "The value for the RETRY COUNT is set to ${RETRY_NUM}"
else
   RETRY_NUM=3
   LOGGER I "The default value for the RETRY COUNT is set to ${RETRY_NUM}"
fi

SLEEP=${SLEEP_TIME}
if  [ -n "${SLEEP}" ]
then
   LOGGER I "The value for the SLEEP TIME is set to ${SLEEP}"
else
   SLEEP=0
   LOGGER I "The default value for the SLEEP TIME is set to ${SLEEP}"
fi

# Here is where we'll finally run SPARK SQL
# It is assumed that SPARK-SUBMIT can be found via PATH
COUNTER=0
   # run Spark-submit
   LOGGER I "Begin - Run Spark-Submit"
   while [ ${COUNTER} -lt ${RETRY_NUM} ]
    do
        if [ ${SPARK_MAJOR_VERSION} = "2" ]
        then
		    LOGGER I "Begin - Run Spark2-Submit"
		    if [ ${EXECUTION_MODE} = "STREAMING" ]
			then
				${SPARK2_SUBMIT} --verbose \
				--master ${SPARK_MASTER} \
				--deploy-mode ${SPARK_DEPLOY_MODE} \
				--queue ${QUEUE} \
				--num-executors ${NO_OF_EXECUTERS} \
				--executor-memory ${EXECUTOR_MEMORY} \
				--executor-cores ${EXECUTOR_CORES} \
				--driver-memory ${DRIVER_MEMORY} \
				--files "${JAAS_FILE},${KEYTAB_FILE}" \
				--conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=${JAAS_FILE_NAME}" \
				--driver-java-options "-Djava.security.auth.login.config=${JAAS_FILE}" \
				--class ${DRIVER_CLASS_NAME} \
				--jars ${JAR_FILES} \
				${SPARK2_DRIVER_JAR} \
				appName=${APP_NAME} \
				sparkMaster=${SPARK_MASTER} \
				inifile=${INI_FILE} \
				job=${INI_SECTION} \
				1>>${LOG_FILE}.tmp 2>&1
			else
				${SPARK2_SUBMIT} --verbose \
				--queue ${QUEUE} \
				--num-executors ${NO_OF_EXECUTERS} \
				--executor-memory ${EXECUTOR_MEMORY} \
				--executor-cores ${EXECUTOR_CORES} \
				--driver-memory ${DRIVER_MEMORY} \
				--class ${DRIVER_CLASS_NAME} \
				--jars ${JAR_FILES} \
				${SPARK2_DRIVER_JAR} \
				appName=${APP_NAME} \
				sparkMaster=${SPARK_MASTER} \
				inifile=${INI_FILE} \
				job=${INI_SECTION} \
				1>>${LOG_FILE}.tmp 2>&1
			fi
        else
            LOGGER I "Begin - Run Spark1-Submit"
			if [ ${EXECUTION_MODE} = "STREAMING" ]
			then
				${SPARK_SUBMIT} --verbose \
				--master ${SPARK_MASTER} \
				--deploy-mode ${SPARK_DEPLOY_MODE} \
				--queue ${QUEUE} \
				--num-executors ${NO_OF_EXECUTERS} \
				--executor-memory ${EXECUTOR_MEMORY} \
				--executor-cores ${EXECUTOR_CORES} \
				--driver-memory ${DRIVER_MEMORY} \
				--files "${JAAS_FILE},${KEYTAB_FILE}" \
				--conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=${JAAS_FILE_NAME}" \
				--driver-java-options "-Djava.security.auth.login.config=${JAAS_FILE}" \
				--class ${DRIVER_CLASS_NAME} \
				--jars ${JAR_FILES} \
				${SPARK1_DRIVER_JAR} \
				appName=${APP_NAME} \
				sparkMaster=${SPARK_MASTER} \
				inifile=${INI_FILE} \
				job=${INI_SECTION} \
				1>>${LOG_FILE}.tmp 2>&1
			else
			    ${SPARK_SUBMIT} --verbose \
				--queue ${QUEUE} \
				--num-executors ${NO_OF_EXECUTERS} \
				--executor-memory ${EXECUTOR_MEMORY} \
				--executor-cores ${EXECUTOR_CORES} \
				--driver-memory ${DRIVER_MEMORY} \
				--class ${DRIVER_CLASS_NAME} \
				--jars ${JAR_FILES} \
				${SPARK1_DRIVER_JAR} \
				appName=${APP_NAME} \
				sparkMaster=${SPARK_MASTER} \
				inifile=${INI_FILE} \
				job=${INI_SECTION} \
				1>>${LOG_FILE}.tmp 2>&1
			fi
        fi
      SPARK_RETCODE=${?}
      LOGGER I "Spark Submit return code: ${SPARK_RETCODE}"
      if [ ${SPARK_RETCODE} -eq 0 ]
      then
         # this .tmp log is to ensure that we capture all of the Spark-Submit output to the log
         LOGGER I "`cat ${LOG_FILE}.tmp`i"
         rm -f ${LOG_FILE}.tmp
         LOGGER I "Running Spark-Submit is succesfully completed"
         break
      else
         LOGGER E "********** ERROR **********"
         LOGGER E "Something went wrong in running the Spark-Submit, exiting"
         LOGGER I "`cat ${LOG_FILE}.tmp`i"
         rm -f ${LOG_FILE}.tmp
         let COUNTER=${COUNTER}+1
         let ATTEMPT=${COUNTER}+1

      fi
      if [ ${COUNTER} -ge ${RETRY_NUM} ]
      then
         LOGGER E "********** ERROR **********"
                 LOGGER E " Max number of retries: ${RETRY_NUM} reached. Research the error, and correct the condition before restarting"
         LOGGER E "Spark-Submit exited with a non-zero exit code: ${SPARK_RET_CODE}"
         LOGGER E "Be cognizant of any data which may have already processed prior"
         LOGGER E "to the error (be careful not to rerun incorrectly"
         LOGGER E "and potentially duplicating data)"
         FAILURE 126
      fi
         LOGGER I "RETRY ATTEMPT :  ${ATTEMPT}"
         sleep ${SLEEP}

   done
   LOGGER I "End - Run Spark-Submit"



# Processing has completed successfully, remove the lock file
#LOGGER I "Begin - removing LOCK FILE ${LOCK_FILE}"
#rm -f ${LOCK_FILE} >/dev/null
#if [ ${?} -ne 0 ]
#then
#   LOGGER E "********** ERROR **********"
#   LOGGER E "could not remove LOCK_FILE ${LOCK_FILE}"
#   FAILURE 116
#fi
#LOGGER I "End - removing LOCK FILE ${LOCK_FILE}"


# clean up old log files from the log archive directory
LOGGER I "Begin - Removing ${JOB_NAME} log files from ${LOG_DIR}/archive older than ${LOG_RETAIN_DAYS} days"
for X in `find "${LOG_DIR}/archive" "${PROGNAMESHORT}_${JOB_NAME}*" -mtime +${LOG_RETAIN_DAYS}`; do
   LOGGER I "removing file: ${X}"
   rm -f ${X}
   if [ ${?} -ne 0 ]
   then
      LOGGER E "********** ERROR **********"
      LOGGER E "Error removing file ${X}"
      FAILURE 105
   fi
done
LOGGER I "End - Removing ${JOB_NAME} log files from ${LOG_DIR}/archive older than ${LOG_RETAIN_DAYS} days"   

LOGGER I "Close log file and move into tar: ${LOG_DIR}/archive/${PROGNAMESHORT}_${JOB_NAME}_`date +%Y%m%d`.tar"
LOGGER I "********** END PROCESS **********"

# Upon successful completion of this script, we'll move the log file to the archive directory
# Becuase we're finalizing the logging, the last cleanup step will be done without logging
# Because multiple threads of this process may multiple times per day,
# the logs in the archive directory will be maintained in a tar ball by date
cd ${LOG_DIR}
gzip `basename ${LOG_FILE}` 
tar -rf ${LOG_DIR}/archive/${PROGNAMESHORT}_${JOB_NAME}_`date +%Y%m%d`.tar `basename ${LOG_FILE}`.gz
if [ ${?} -eq 0 ]
then
   # Remove the untarred log file once it's been successfully added to the archive tar
   # if for some reason the tar command fails, the untarred log file will remain in LOG_DIR
   rm -f ${LOG_FILE}.gz
fi

exit 0

------------------
------------
kinit -kt /home/ebdpcedms/ebdpcedms.keytab ebdpcedms@CABLE.COMCAST.COM
---------
----------kafka_client_cedm_jaas.conf--
KafkaClient {
     com.sun.security.auth.module.Krb5LoginModule required
     useKeyTab=true
     keyTab="/home/ebdpcedms/ebdpcedms.keytab"
     storeKey=true
     useTicketCache=false
     serviceName="kafka"
     principal="ebdpcedms@CABLE.COMCAST.COM";
    };
---------------------
------------------kafka_client_jaas_spark.conf
KafkaClient {
  com.sun.security.auth.module.Krb5LoginModule required
  useTicketCache=true
  useKeyTab=true
  principal="ebdpcedms@CABLE.COMCAST.COM"
  keyTab="ebdpcedms.keytab"
  serviceName="kafka"
  renewTicket=true
  client=true;
};
Client {
  com.sun.security.auth.module.Krb5LoginModule required
  useTicketCache=true
  useKeyTab=true
  principal="ebdpcedms@CABLE.COMCAST.COM"
  keyTab="ebdpcedms.keytab"
  renewTicket=true
  serviceName="zookeeper";
};
---------------
--------------stop_catena_spark_streaming_job.sh---
#!/bin/bash
echo "Receive SPARK job config $1"

appName=`cat $1 | grep 'APP_NAME' | cut -d '=' -f2`
echo "appName: "$appName""
appName1=`cat $1 | grep 'DRIVER_CLASS_NAME' | cut -d '=' -f2`
echo "appName1: "$appName1""
sparkMajorVersion=`cat $1 | grep 'SPARK_MAJOR_VERSION' | cut -d '=' -f2`
echo "sparkMajorVersion: "$sparkMajorVersion""

user=`whoami`
echo "user: "$user""
kinit -kt /home/`whoami`/`whoami`.keytab `whoami`@CABLE.COMCAST.COM

#check if application not running with same name else stop the job
if [ ${SPARK_MAJOR_VERSION} = "2" ]
then
application_id=$(yarn application -list | grep -w "${appName}" | grep -oe "application_[0-9]*_[0-9]*")
else
application_id=$(yarn application -list | grep -w "${appName1}" | grep -oe "application_[0-9]*_[0-9]*")
fi
application_status=$(yarn application -status ${application_id} | grep "State : \(RUNNING\|ACCEPTED\)"|awk -F" " '{print $3}')
echo "application_status: "$application_status""

if [ $application_status == "RUNNING" ] || [ $application_status == "ACCEPTED" ]
then
        echo ""$appName" Spark Streaming Job currently running"
        echo "Stopping "$appName" Spark Streaming Job"

       yarn application -kill ${application_id}

    if [ ${?} -ne 0 ]
    then
        echo "********** ERROR **********"
        echo "Spark Streaming "$appName" Job failed to stop"
    else
        echo "Spark Streaming "$appName" Job stopped successfully"
    fi
elif [ -z $application_id ]
then
    echo ""$appName" Spark Streaming Job is currently not running"
else
    echo ""$appName" Spark Streaming Job has in strange status, please check!!"
fi
------------
------------------Spark_sample_script.txt------
! PSET sets a value to an object variable, which can be passed to a CALL
:PSET &OBJECT_ALIAS# = &$ALIAS#
! Print the value to Activation report, just for debugging to see that we are capturing the expected value (can omit, or remove once you see it's working)
:PRINT &OBJECT_ALIAS#

cd /home/`whoami`/jobs/data_ingestion/ebds_meld/jars
export SPARK_JARS2=ssl-config-akka_2.11-0.2.1.jar,ssl-config-core_2.11-0.2.1.jar,config-1.3.0.jar,jersey-client-1.19.1.jar,akka-actor_2.11-2.4.8.jar,akka-http-experimental_2.11-2.4.8.jar,akka-parsing_2.11-2.4.8.jar,akka-http-core_2.11-2.4.8.jar,akka-stream_2.11-2.4.8.jar,jackson-core-2.8.1.jar,json4s-core_2.11-3.2.11.jar,json4s-ast_2.11-3.2.11.jar,json4s-jackson_2.11-3.2.11.jar,json4s-native_2.11-3.2.11.jar,kafka_2.11-0.8.2.2.jar,scalatest_2.11-3.0.0.jar,spark-core_2.11-2.0.2-SNAPSHOT.jar,spark-hive_2.11-2.0.2-SNAPSHOT.jar,spark-mllib_2.11-2.0.2-SNAPSHOT.jar,spark-sql_2.11-2.0.2-SNAPSHOT.jar,spark-streaming_2.11-2.0.2-SNAPSHOT.jar,spark-streaming-kafka_2.11-1.6.2.jar,spark-testing-base_2.11-2.0.0_0.4.4.jar,spark-yarn_2.11-2.0.2-SNAPSHOT.jar,spray-json_2.11-1.3.2.jar,metrics-core-2.2.0.jar,reactive-streams-1.0.0.jar,akka-slf4j_2.11-2.4.8.jar,akka-http-cors_2.11-0.1.4.jar,scopt_2.11-3.5.0.jar,json-1.3.7.3.jar,json-serde-1.3.7.3.jar,json-serde-cdh5-shim-1.3.7.3.jar

STARTDAY=`date +"%Y%m%d"`
ENDDAY=`date +"%Y%m%d"`
EBDSJOB=agent_level_cos
EXECUTORS_NUM=20
EXECUTORS_MEM=4G

LOGNAME=$EBDSJOB\_`date +"%Y%m%d_%H_%M_%S"`
EBDSLOG=/home/`whoami`/jobs/data_ingestion/ebds_meld/logs/$LOGNAME.log
echo "Submitting the Spark job to the YARN"
/usr/hdp/current/spark2-client/bin/spark-submit --verbose --master yarn --deploy-mode client --conf spark.yarn.submit.waitAppCompletion=true --queue=&QUEUE --num-executors $EXECUTORS_NUM  --executor-memory $EXECUTORS_MEM --executor-cores 1 --driver-memory 8G  --class com.comcast.tds.spark.ebds.core.Driver --jars $SPARK_JARS2 /home/`whoami`/jobs/data_ingestion/ebds_meld/jars/ebds_meld_2.11-0.1_account.jar --config=/db/everest/ebdsmeld.config --appName=EBDS_MELD --sparkMaster=yarnclient --job=$EBDSJOB --days=$STARTDAY,$ENDDAY >> $EBDSLOG 2>&1
ret=${?}
if [ ${ret} -ne 0 ]
then
echo "Error - Please check the log file"
exit ${ret}
else
echo "Job completed successfully"
exit 0
fi



/usr/hdp/current/spark2-client/bin/spark-submit --verbose --master yarn --deploy-mode client --conf spark.yarn.submit.waitAppCompletion=true --queue= --num-executors $EXECUTORS_NUM  --executor-memory $EXECUTORS_MEM --executor-cores 1 --driver-memory 8G  --class com.comcast.tds.spark.ebds.core.Driver --jars $SPARK_JARS2 /home/`whoami`/jobs/data_ingestion/ebds_meld/jars/ebds_meld_2.11-0.1_account.jar --config=/db/everest/ebdsmeld.config --appName=EBDS_MELD --sparkMaster=yarnclient --job=$EBDSJOB --days=$STARTDAY,$ENDDAY

export SPARKLE_JARS2=/home/nkota200/jars/ssl-config-akka_2.11-0.2.1.jar,/home/nkota200/jars/ssl-config-core_2.11-0.2.1.jar,/home/nkota200/jars/config-1.3.0.jar,/home/nkota200/jars/jersey-client-1.19.1.jar,/home/nkota200/jars/akka-actor_2.11-2.4.8.jar,/home/nkota200/jars/akka-http-experimental_2.11-2.4.8.jar,/home/nkota200/jars/akka-parsing_2.11-2.4.8.jar,/home/nkota200/jars/akka-http-core_2.11-2.4.8.jar,/home/nkota200/jars/akka-stream_2.11-2.4.8.jar,/home/nkota200/jars/jackson-core-2.8.1.jar,/home/nkota200/jars/json4s-core_2.11-3.2.11.jar,/home/nkota200/jars/json4s-ast_2.11-3.2.11.jar,/home/nkota200/jars/json4s-jackson_2.11-3.2.11.jar,/home/nkota200/jars/json4s-native_2.11-3.2.11.jar,/home/nkota200/jars/kafka_2.11-0.8.2.2.jar,/home/nkota200/jars/scalatest_2.11-3.0.0.jar,/home/nkota200/jars/spark-core_2.11-2.0.2-SNAPSHOT.jar,/home/nkota200/jars/spark-hive_2.11-2.0.2-SNAPSHOT.jar,/home/nkota200/jars/spark-mllib_2.11-2.0.2-SNAPSHOT.jar,/home/nkota200/jars/spark-sql_2.11-2.0.2-SNAPSHOT.jar,/home/nkota200/jars/spark-streaming_2.11-2.0.2-SNAPSHOT.jar,/home/nkota200/jars/spark-streaming-kafka_2.11-1.6.2.jar,/home/nkota200/jars/spark-testing-base_2.11-2.0.0_0.4.4.jar,/home/nkota200/jars/spark-yarn_2.11-2.0.2-SNAPSHOT.jar,/home/nkota200/jars/spray-json_2.11-1.3.2.jar,/home/nkota200/jars/metrics-core-2.2.0.jar,/home/nkota200/jars/reactive-streams-1.0.0.jar,/home/nkota200/jars/akka-slf4j_2.11-2.4.8.jar,/home/nkota200/jars/akka-http-cors_2.11-0.1.4.jar,/home/nkota200/jars/scopt_2.11-3.5.0.jar,/home/nkota200/jars/json-1.3.7.3.jar,/home/nkota200/jars/json-serde-1.3.7.3.jar,/home/nkota200/jars/json-serde-cdh5-shim-1.3.7.3.jar
YEAR=2017
STARTDAY=${YEAR}1115
ENDDAY=${YEAR}1115
EBDSJOB=agent_level_cos
EXECUTORS_NUM=40
EXECUTORS_MEM=4G
#submit the below commands AS IS after modifying the job parameters
LOGNAME=$EBDSJOB\_$STARTDAY-$ENDDAY\_`date +"%Y%m%d_%H_%M_%S"`
EBDSLOG=/home/ebdpcedms/$LOGNAME.log
nohup /usr/hdp/current/spark2-client/bin/spark-submit --verbose --master yarn --deploy-mode client --queue=meld --num-executors $EXECUTORS_NUM  --executor-memory $EXECUTORS_MEM --executor-cores 1 --driver-memory 8G  --class com.comcast.tds.spark.ebds.core.Driver --jars $SPARKLE_JARS2 /home/ebdpcedms/ebds_meld_2.11-0.1_agent.jar  --appName=EBDS_MELD --sparkMaster=yarnclient --job=$EBDSJOB --days=$STARTDAY,$ENDDAY >> $EBDSLOG 2>&1&

cd /home/`whoami`/jobs/data_ingestion/ebds_meld/jars
export SPARK_JARS2=ssl-config-akka_2.11-0.2.1.jar,ssl-config-core_2.11-0.2.1.jar,config-1.3.0.jar,jersey-client-1.19.1.jar,akka-actor_2.11-2.4.8.jar,akka-http-experimental_2.11-2.4.8.jar,akka-parsing_2.11-2.4.8.jar,akka-http-core_2.11-2.4.8.jar,akka-stream_2.11-2.4.8.jar,jackson-core-2.8.1.jar,json4s-core_2.11-3.2.11.jar,json4s-ast_2.11-3.2.11.jar,json4s-jackson_2.11-3.2.11.jar,json4s-native_2.11-3.2.11.jar,kafka_2.11-0.8.2.2.jar,scalatest_2.11-3.0.0.jar,spark-core_2.11-2.0.2-SNAPSHOT.jar,spark-hive_2.11-2.0.2-SNAPSHOT.jar,spark-mllib_2.11-2.0.2-SNAPSHOT.jar,spark-sql_2.11-2.0.2-SNAPSHOT.jar,spark-streaming_2.11-2.0.2-SNAPSHOT.jar,spark-streaming-kafka_2.11-1.6.2.jar,spark-testing-base_2.11-2.0.0_0.4.4.jar,spark-yarn_2.11-2.0.2-SNAPSHOT.jar,spray-json_2.11-1.3.2.jar,metrics-core-2.2.0.jar,reactive-streams-1.0.0.jar,akka-slf4j_2.11-2.4.8.jar,akka-http-cors_2.11-0.1.4.jar,scopt_2.11-3.5.0.jar,json-1.3.7.3.jar,json-serde-1.3.7.3.jar,json-serde-cdh5-shim-1.3.7.3.jar

STARTDAY=`date +"%Y%m%d"`
ENDDAY=`date +"%Y%m%d"`
EBDSJOB=epcLookup
EXECUTORS_NUM=50
EXECUTORS_MEM=4G

LOGNAME=$EBDSJOB\_`date +"%Y%m%d_%H_%M_%S"`
EBDSLOG=/home/`whoami`/jobs/data_ingestion/ebds_meld/logs/$LOGNAME.log
echo "Submitting the Spark job to the YARN"
/usr/hdp/current/spark2-client/bin/spark-submit --verbose --master yarn --deploy-mode client --conf spark.yarn.submit.waitAppCompletion=true --queue=&QUEUE --num-executors $EXECUTORS_NUM  --executor-memory $EXECUTORS_MEM --executor-cores 1 --driver-memory 8G  --class com.comcast.tds.spark.ebds.core.Driver --jars $SPARK_JARS2 /home/`whoami`/jobs/data_ingestion/ebds_meld/jars/ebds_meld_2.11-0.1_account.jar --config=/db/everest/ebdsmeld.config --appName=EBDS_MELD --sparkMaster=yarnclient --job=$EBDSJOB --days=$STARTDAY,$ENDDAY >> $EBDSLOG 2>&1
ret=${?}
if [ ${ret} -ne 0 ]
then
echo "Error - Please check the log file"
exit ${ret}
else
echo "Job completed successfully"
exit 0
fi

/home/EBDPINTEVRS/jobs/data_ingestion/ebds_meld/config/ebds_meld_distctp_core_to_hood.ini

EVEREST_EBDS_MELD_DEVICE_LOAD_DAILY

cd /home/`whoami`/jobs/data_ingestion/ebds_meld
./distcp_core_to_hood_wrapper.sh -i &INI -s &SECTION


--HDP 2.6 Stage
nohup /usr/hdp/current/spark2-client/bin/spark-submit --verbose --master yarn --deploy-mode client --queue=meld --num-executors 20  --executor-memory 4G --executor-cores 1 --driver-memory 8G  --class com.comcast.meld.catena.Driver /home/ebdpcedms/jobs/data_ingestion/catena/catena_2.11-1.0.jar appName=CATENA sparkMaster=yarnclient inifile=/home/ebdpcedms/jobs/data_ingestion/catena/ini/CTP_Summary_Reports.ini job=SUMMARY_CTP_ORDERS >> /home/ebdpcedms/jobs/data_ingestion/catena/log/test4.log 2>&1&


nohup spark-submit --verbose --master yarn --deploy-mode client --queue=catena_highsla --num-executors 10  --executor-memory 4G --executor-cores 1 --driver-memory 8G  --class com.comcast.meld.catena.Driver /home/ebdpcedms/jobs/data_ingestion/catena/catena_2.10-1.0.jar appName=CATENA sparkMaster=yarnclient inifile=/home/ebdpcedms/jobs/data_ingestion/catena/ini/CTP_Summary_Reports.ini job=SUMMARY_CTP_ORDERS >> /home/ebdpcedms/jobs/data_ingestion/catena/log/test.log 2>&1&

--PROD
nohup spark-submit --verbose --master yarn --deploy-mode client --num-executors 10  --executor-memory 4G --executor-cores 1 --driver-memory 1G  --class com.comcast.meld.catena.Driver /home/nmalli002c/catena/catena_2.10-1.0.jar appName=CATENA sparkMaster=yarnclient inifile=/home/nmalli002c/catena/ini/CTP_Summary_Reports.ini job=SUMMARY_CTP_ORDERS >> /home/nmalli002c/catena/log/test.log 2>&1&

Spark 1.6:

nohup spark-submit --verbose --num-executors 10  --executor-memory 4G --executor-cores 1 --driver-memory 1G --class com.comcast.meld.catena.core.Driver /home/nmalli002c/catena/catena_spark_2.10-1.0.jar appName=CATENA sparkMaster=yarnclient inifile=/home/nmalli002c/catena/ini/CTP_Summary_Reports.ini job=SUMMARY_CTP_ORDERS >> /home/nmalli002c/catena/log/summary_ctp_orders.log 2>&1&

nohup spark-submit --verbose --master yarn --deploy-mode client --num-executors 10  --executor-memory 4G --executor-cores 1 --driver-memory 1G  --class com.comcast.meld.catena.core.Driver /home/nmalli002c/catena/catena_spark_2.10-1.0.jar appName=CATENA sparkMaster=yarnclient inifile=/home/nmalli002c/catena/ini/CTP_Summary_Reports.ini job=SUMMARY_CTP_ORDER_ITEM >> /home/nmalli002c/catena/log/summary_ctp_order_item.log 2>&1&

nohup spark-submit --verbose --master yarn --deploy-mode client --num-executors 10  --executor-memory 4G --executor-cores 1 --driver-memory 1G  --class com.comcast.meld.catena.core.Driver /home/nmalli002c/catena/catena_spark_2.10-1.0.jar appName=CATENA sparkMaster=yarnclient inifile=/home/nmalli002c/catena/ini/CTP_Summary_Reports.ini job=SUMMARY_OPPORTUNITY_DETAIL >> /home/nmalli002c/catena/log/summary_opportunity_detail.log 2>&1&

Spark 2.2:
/home/nmalli002c/catena/jars/config-1.3.1.jar

nohup /home/nmalli002c/spark-2.2.0-bin-hadoop2.7_new/bin/spark-submit --verbose --master yarn --deploy-mode client --num-executors 10  --executor-memory 4G --executor-cores 1 --driver-memory 1G  --class com.comcast.meld.catena.core.Driver --jars /home/nmalli002c/catena/jars/config-1.3.1.jar /home/nmalli002c/catena/catena_spark_2_2.11-1.0.jar appName=CATENA sparkMaster=yarnclient inifile=/home/nmalli002c/catena/ini/CTP_Summary_Reports.ini job=SUMMARY_CTP_ORDERS >> /home/nmalli002c/catena/log2/summary_ctp_orders.log 2>&1&

nohup /home/nmalli002c/spark-2.2.0-bin-hadoop2.7_new/bin/spark-submit --verbose --master yarn --deploy-mode client --num-executors 10  --executor-memory 4G --executor-cores 1 --driver-memory 1G  --class com.comcast.meld.catena.core.Driver /home/nmalli002c/catena/catena_spark_2_2.11-1.0.jar appName=CATENA sparkMaster=yarnclient inifile=/home/nmalli002c/catena/ini/CTP_Summary_Reports.ini job=SUMMARY_CTP_ORDERS >> /home/nmalli002c/catena/log2/summary_ctp_orders.log 2>&1&

nohup /home/nmalli002c/spark-2.2.0-bin-hadoop2.7_new/bin/spark-submit --verbose --master yarn --deploy-mode client --num-executors 10  --executor-memory 4G --executor-cores 1 --driver-memory 1G  --class com.comcast.meld.catena.core.Driver /home/nmalli002c/catena/catena_spark_2_2.11-1.0.jar appName=CATENA sparkMaster=yarnclient inifile=/home/nmalli002c/catena/ini/CTP_Summary_Reports.ini job=SUMMARY_CTP_ORDER_ITEM >> /home/nmalli002c/catena/log2/summary_ctp_order_item.log 2>&1&

nohup /home/nmalli002c/spark-2.2.0-bin-hadoop2.7_new/bin/spark-submit --verbose --master yarn --deploy-mode client --num-executors 10  --executor-memory 4G --executor-cores 1 --driver-memory 1G  --class com.comcast.meld.catena.core.Driver /home/nmalli002c/catena/catena_spark_2_2.11-1.0.jar appName=CATENA sparkMaster=yarnclient inifile=/home/nmalli002c/catena/ini/CTP_Summary_Reports.ini job=SUMMARY_OPPORTUNITY_DETAIL >> /home/nmalli002c/catena/log2/summary_opportunity_detail.log 2>&1&

cd /home/nmalli002c/jobs/data_ingestion/shell
nohup /home/nmalli002c/jobs/data_ingestion/shell/catena_spark_sql_wrapper.sh -i /home/nmalli002c/jobs/data_ingestion/ini/ctp_summary_reports.ini -s SUMMARY_CTP_ORDERS -r /home/nmalli002c/jobs/data_ingestion/runtimefiles/45921_CEDM_CTP_ORDERS.ini&

nohup /home/nmalli002c/jobs/data_ingestion/shell/catena_spark_sql_wrapper.sh -i /home/nmalli002c/jobs/data_ingestion/ini/ctp_summary_reports.ini -s SUMMARY_CTP_ORDER_ITEM -r /home/nmalli002c/jobs/data_ingestion/runtimefiles/45921_CEDM_CTP_ORDERS.ini&

nohup /home/nmalli002c/jobs/data_ingestion/shell/catena_spark_sql_wrapper.sh -i /home/nmalli002c/jobs/data_ingestion/ini/ctp_summary_reports.ini -s SUMMARY_OPPORTUNITY_DETAIL -r /home/nmalli002c/jobs/data_ingestion/runtimefiles/45921_CEDM_CTP_ORDERS.ini&


select day_id, count(*) from nmalli002c.summary_ctp_orders group by day_id order by day_id;



select day_id, count(*) from nmalli002c.summary_ctp_order_item group by day_id order by day_id;



select day_id, count(*) from nmalli002c.summary_opportunity_detail group by day_id order by day_id;




scp -r nmalli002c@ebdp-ch2-e024p.sys.comcast.net:/home/nmalli002c/nohup.out /home/nmalli002c/test.out

scp -r nmalli002c@ebdp-ch2-e024p.sys.comcast.net:/home/nmalli002c/spark-2.2.0-bin-hadoop2.7 /home/nmalli002c/spark-2.2.0-bin-hadoop2.7

scp -r nmalli002c@ebdp-ch2-c009s.sys.comcast.net:/home/nmalli002c/spark-2.2.0-bin-hadoop2.7 /home/ebdpcedms/spark-2.2.0-bin-hadoop2.7

cd /home/`whoami`/jobs/data_ingestion/ebds_meld
./distcp_core_to_hood_wrapper.sh -i &INI -s &SECTION

hadoop distcp -pb hdfs://comcastprodcluster/db/catena/edw/core/ hdfs://ebdp-ch2-d035s.sys.comcast.net:8020/db/catena/edw/core/work_order_v/

cp -R /home/nmalli002c/ExtractFramework /home/ebdpcedms/ExtractFramework


./kafka-topics.sh --zookeeper ebdp-ch2-d033s.sys.comcast.net:2181,ebdp-ch2-d041s.sys.comcast.net:2181,ebdp-ch2-d043s.sys.comcast.net:2181 --list

/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --zookeeper ebdp-ch2-d033s.sys.comcast.net:2181 --replication-factor 2 --partitions 2 --topic CEDM_TOLAM_SOURCE_ISSUE

/usr/hdp/current/kafka-broker/bin/kafka-run-class.sh kafka.admin.ConsumerGroupCommand --list --zookeeper ebdp-ch2-d033s.sys.comcast.net:2181

/usr/hdp/current/kafka-broker/bin/kafka-run-class.sh kafka.admin.ConsumerGroupCommand --zookeeper ebdp-ch2-d033s.sys.comcast.net:2181 --describe --group xcc-recon-worklist-qa-grp

/usr/hdp/current/kafka-broker/bin/kafka-consumer-offset-checker.sh --group xcc-recon-worklist-qa-grp --topic CEDM_TOLAM_SOURCE_ISSUE_V1 --zookeeper ebdp-ch2-d033s.sys.comcast.net:2181

/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --zookeeper ebdp-ch2-d002d.sys.comcast.net:2181 --alter --topic CEDM_TAS_DATAINGEST_Q3 --config max.message.bytes=10000000

/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --zookeeper ebdp-ch2-d033s.sys.comcast.net:2181 --topic caapkerberostest --from-beginning

/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --topic catenatas_v0 --zookeeper ebdp-ch2-k001p.sys.comcast.net:2181,ebdp-ch2-k002p.sys.comcast.net:2181 --from-beginning  -security-protocol PLAINTEXTSASL
/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --zookeeper ebdp-ch2-k001p:2181 --describe --topic x1_stb_raw_events_historical_reprocessing_1_v0
/usr/hdp/current/kafka-broker/binkafka-topics.sh --list --zookeeper ebdp-ch2-k004p.sys.comcast.net:2181

/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --alter --topic x1_tuning_intervals_historical_reprocessing_3_v0 --config retention.ms=432000000 --zookeeper ebdp-ch2-k001p.sys.comcast.net:2181


/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --topic caapkerberostest --zookeeper ebdp-ch2-d033s.sys.comcast.net:2181,ebdp-ch2-d041s.sys.comcast.net:2181,ebdp-ch2-d043s.sys.comcast.net:2181 --from-beginning  -security-protocol PLAINTEXTSASL

/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --zookeeper ebdp-ch2-d033s.sys.comcast.net:2181,ebdp-ch2-d041s.sys.comcast.net:2181,ebdp-ch2-d043s.sys.comcast.net:2181 --describe --topic caapkerberostest

sh /usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --new-consumer --bootstrap-server consumer-kafka.hdw.r53.deap.tv:9092 --topic raw.mirrored.xre.nopartner.rawmetriclogs.xreAll  | grep 'ACTIVATION_ERRORS_117'

/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --alter --topic CEDMTOLAMBATCHINGEST_V1 --config retention.ms= --zookeeper ebdp-ch2-k001p.sys.comcast.net:2181

/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --topic CEDMTOLAMBATCHINGEST_V1 --zookeeper ebdp-ch2-k001p.sys.comcast.net:2181,ebdp-ch2-k002p.sys.comcast.net:2181 --from-beginning -security-protocol PLAINTEXTSASL


/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --topic caapingest_v0 --zookeeper ebdp-ch2-k001p.sys.comcast.net:2181,ebdp-ch2-k002p.sys.comcast.net:2181 --from-beginning   -security-protocol PLAINTEXTSASL

/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --topic caapingest_v0 --zookeeper ebdp-ch2-k001p.sys.comcast.net:2181,ebdp-ch2-k002p.sys.comcast.net:2181 --from-beginning   -security-protocol PLAINTEXTSASL | grep 'activityLog'

/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --topic caapingest_v0 --zookeeper ebdp-ch2-k001p.sys.comcast.net:2181,ebdp-ch2-k002p.sys.comcast.net:2181 --from-beginning   -security-protocol PLAINTEXTSASL | grep 'activationRecord'

/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --topic caapingest_v0 --zookeeper ebdp-ch2-k001p.sys.comcast.net:2181,ebdp-ch2-k002p.sys.comcast.net:2181 --from-beginning   -security-protocol PLAINTEXTSASL | grep 'device'

/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --topic caapkerberostest --zookeeper ebdp-ch2-d033s.sys.comcast.net:2181,ebdp-ch2-d041s.sys.comcast.net:2181,ebdp-ch2-d043s.sys.comcast.net:2181 --from-beginning -security-protocol PLAINTEXTSASL

/usr/hdp/current/kafka-broker/bin/kafka-console-consumer.sh --topic caapingest_v0 --zookeeper ebdp-ch2-d033s.sys.comcast.net:2181,ebdp-ch2-d041s.sys.comcast.net:2181,ebdp-ch2-d043s.sys.comcast.net:2181 --from-beginning -security-protocol PLAINTEXTSASL | grep '"table"="device"'



/home/ebdpcedms/jobs/data_ingestion/shell/catena_spark_sql_wrapper.sh -i /home/ebdpcedms/jobs/data_ingestion/ini/caap_streaming.ini -s CAAP_TEAM_G_STREAMING -r /home/ebdpcedms/jobs/data_ingestion/runtimefiles/4659_CEDM_CTP_ORDERS.ini

/home/ebdpcedms/jobs/data_ingestion/shell/catena_spark_sql_wrapper.sh -i /home/ebdpcedms/jobs/data_ingestion/ini/hw_streaming.ini -s HW_STREAMING -r /home/ebdpcedms/jobs/data_ingestion/runtimefiles/4659_CEDM_CTP_ORDERS.ini

/home/ebdpcedms/jobs/data_ingestion/shell/catena_spark_streaming_wrapper.sh -i /home/ebdpcedms/jobs/data_ingestion/ini/caap_streaming.ini -s CAAP_TEAM_G_STREAMING -r /home/ebdpcedms/jobs/data_ingestion/runtimefiles/4659_CEDM_CTP_ORDERS.ini

/home/ebdpcedms/jobs/data_ingestion/shell/catena_spark_submit_wrapper.sh -i /home/ebdpcedms/jobs/data_ingestion/ini/caap_streaming.ini -s CAAP_TEAM_G_STREAMING

/home/ebdpcedms/jobs/data_ingestion/shell/stop_catena_spark_streaming_job.sh /home/ebdpcedms/jobs/data_ingestion/ini/caap_streaming.ini

testing:
./spark-submit --verbose --master yarn --deploy-mode client --queue=catena_highsla --num-executors 5  --executor-memory 4G --executor-cores 1 --driver-memory 6G  --class com.comcast.meld.catena.caap_streaming.CAAPTeamGStreamTest --jars /home/ebdpcedms/jobs/data_ingestion/jars/common/config-1.3.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/spark-streaming-kafka-0-8-assembly_2.11-2.2.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/kafka_2.10-0.9.0.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/kafka-clients-0.9.0.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/json4s-jackson_2.11-3.3.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/json4s-core_2.11-3.3.0.jar /home/ebdpcedms/jobs/data_ingestion/jars/CAAP/catena-spark_2.11-1.0.jar >> /home/ebdpcedms/jobs/data_ingestion/caap_spark_streaming_logs/test1.log 2>&1

./spark-submit --verbose --master local[4]  --class com.comcast.meld.catena.caap_streaming.CAAPTeamGStreamTest --jars /home/ebdpcedms/jobs/data_ingestion/jars/common/config-1.3.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/spark-streaming-kafka-0-8-assembly_2.11-2.2.0.jar /home/ebdpcedms/jobs/data_ingestion/jars/CAAP/catena-spark_2.11-1.0.jar >> /home/ebdpcedms/jobs/data_ingestion/caap_spark_streaming_logs/test4.log 2>&1

./spark-submit --verbose --master yarn --deploy-mode client --queue=catena_highsla --num-executors 5  --executor-memory 4G --executor-cores 1 --driver-memory 6G  --class com.comcast.meld.catena.caap_streaming.HeadwatersStreamTest --jars /home/ebdpcedms/jobs/data_ingestion/jars/common/config-1.3.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/spark-streaming-kafka-0-8-assembly_2.11-2.2.0.jar /home/ebdpcedms/jobs/data_ingestion/jars/CAAP/catena-spark_2.11-1.0.jar >> /home/ebdpcedms/jobs/data_ingestion/caap_spark_streaming_logs/test2.log 2>&1

./spark-submit --verbose --master local[4] --class com.comcast.meld.catena.caap_streaming.HeadwatersStreamTest --jars /home/ebdpcedms/jobs/data_ingestion/jars/common/config-1.3.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/spark-streaming-kafka-0-8-assembly_2.11-2.2.0.jar /home/ebdpcedms/jobs/data_ingestion/jars/CAAP/catena-spark_2.11-1.0.jar >> /home/ebdpcedms/jobs/data_ingestion/caap_spark_streaming_logs/test3.log 2>&1

JAR_FILES=/home/ebdpcedms/jobs/data_ingestion/jars/common/config-1.3.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/spark-streaming-kafka-0-8-assembly_2.11-2.2.0.jar

JAR_FILES=/home/ebdpcedms/jobs/data_ingestion/jars/common/config-1.3.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/spark-streaming-kafka_2.10-1.6.2.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/kafka_2.10-0.8.2.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/kafka-clients-0.8.2.1.jar

JAR_FILES=/home/ebdpcedms/jobs/data_ingestion/jars/common/config-1.3.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/spark-streaming-kafka-0-8-assembly_2.11-2.2.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/kafka_2.10-0.9.0.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/kafka-clients-0.9.0.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/json4s-jackson_2.11-3.3.0.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/json4s-core_2.11-3.3.0.jar

1.6.2:
spark-submit --verbose --master local[4] --class com.comcast.meld.catena.caap_streaming.HeadwatersStreamTest --jars /home/ebdpcedms/jobs/data_ingestion/jars/common/config-1.3.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/spark-streaming-kafka_2.10-1.6.2.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/kafka_2.10-0.8.2.1.jar,/home/ebdpcedms/jobs/data_ingestion/jars/common/kafka-clients-0.8.2.1.jar /home/ebdpcedms/jobs/data_ingestion/jars/CAAP/catena-spark1_2.10-1.0.jar >> /home/ebdpcedms/jobs/data_ingestion/caap_spark_streaming_logs/test5.log 2>&1

spark-submit --verbose --master local[4] --class com.comcast.meld.catena.caap_streaming.HeadwatersStreamTest /home/ebdpcedms/jobs/data_ingestion/jars/CAAP/catena-spark1-assembly-1.0.jar >> /home/ebdpcedms/jobs/data_ingestion/caap_spark_streaming_logs/test6.log 2>&1

spark-submit --verbose --master local[4] --files "/home/ebdpcedms/kafka_client_jaas_spark.conf,/home/ebdpcedms/ebdpcedms.keytab" --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=kafka_client_jaas_spark.conf" --driver-java-options "-Djava.security.auth.login.config=/home/ebdpcedms/kafka_client_jaas_spark.conf" --class com.comcast.meld.catena.caap_streaming.CAAPTeamGStreamTest /home/ebdpcedms/jobs/data_ingestion/jars/CAAP/catena-spark1-assembly-1.0.jar >> /home/ebdpcedms/jobs/data_ingestion/caap_spark_streaming_logs/test9.log 2>&1

spark-submit --verbose --master yarn --deploy-mode client --queue=catena_highsla --num-executors 5  --executor-memory 4G --executor-cores 1 --driver-memory 4G --files "/home/ebdpcedms/kafka_client_jaas_spark.conf,/home/ebdpcedms/ebdpcedms.keytab" --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=kafka_client_jaas_spark.conf" --driver-java-options "-Djava.security.auth.login.config=/home/ebdpcedms/kafka_client_jaas_spark.conf" --class com.comcast.meld.catena.caap_streaming.CAAPTeamGStreamTest1 /home/ebdpcedms/jobs/data_ingestion/jars/CAAP/catena-spark1-assembly-1.0.jar >> /home/ebdpcedms/jobs/data_ingestion/caap_spark_streaming_logs/test11.log 2>&1

--files "/home/ebdpcedms/kafka_client_jaas_spark.conf,/home/ebdpcedms/ebdpcedms.keytab" \
--conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=kafka_client_jaas_spark.conf" \
--driver-java-options "-Djava.security.auth.login.config=/home/ebdpcedms/kafka_client_jaas_spark.conf" \

spark-shell --master yarn-client --jars /home/nmalli002c/kafka/spark-streaming-kafka_2.10-1.6.2.jar 

spark-shell --master yarn --deploy-mode client --num-executors 5  --executor-memory 4G --executor-cores 1 --driver-memory 2G --jars /home/nmalli002c/kafka/spark-streaming-kafka_2.10-1.6.2.jar

spark-shell --master yarn --deploy-mode client --num-executors 5  --executor-memory 4G --executor-cores 1 --driver-memory 2G --jars /home/ebdpcedms/jars_1/spark-streaming-kafka_2.10-1.6.2.jar,/home/ebdpcedms/jars_1/scalatest_2.10-2.2.1.jar,/home/ebdpcedms/jars_1/scalacheck_2.10-1.11.3.jar,/home/ebdpcedms/jars_1/spark-test-tags_2.10-1.6.2.jar,/home/ebdpcedms/jars_1/spark-core_2.10-1.6.2.jar,/home/ebdpcedms/jars_1/jopt-simple-3.2.jar,/home/ebdpcedms/jars_1/junit-4.11.jar,/home/ebdpcedms/jars_1/junit-interface-0.11.jar,/home/ebdpcedms/jars_1/spark-streaming_2.10-1.6.2.jar,/home/ebdpcedms/jars_1/kafka_2.10-0.8.2.1.jar,/home/ebdpcedms/jars_1/spark-streaming-kafka_2.10-1.6.2.jar

spark-shell --verbose --master yarn --deploy-mode client --num-executors 5  --executor-memory 4G --executor-cores 1 --driver-memory 2G --packages org.apache.spark:spark-streaming-kafka_2.10:1.6.1

cd /home/ebdpcedmp/spark-2.2.0-bin-hadoop2.7/bin
./spark-shell --master yarn --deploy-mode client --num-executors 5  --executor-memory 4G --executor-cores 1 --driver-memory 2G --jars /home/nmalli002c/kafka/spark-streaming-kafka-0-8_2.11-2.2.0.jar

cd /home/ebdpcedmp/spark-2.2.0-bin-hadoop2.7/bin
./spark-shell --master yarn --deploy-mode client --num-executors 5  --executor-memory 4G --executor-cores 1 --driver-memory 2G --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0


	
---------------------

