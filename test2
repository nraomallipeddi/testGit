import org.apache.hadoop.fs.{ FileSystem, Path }

def isPathExist(hdfsLocation: String): Boolean = {
    val conf = Utils.sc.hadoopConfiguration
    val fs = FileSystem.get(conf)
    fs.exists(new Path(hdfsLocation))
  }

  def containsFiles(hdfsLocation: String): Boolean = {
    val conf = sc.hadoopConfiguration
    val fs = FileSystem.get(conf)
    var ret = fs.exists(new Path(hdfsLocation))

    if (ret) {
      ret = false
      val iter = fs.listFiles(new Path(hdfsLocation), false)
      while (iter.hasNext()) {
        val f = iter.next().getPath().getName
        if (!f.equals("_SUCCESS")) {
          ret = true
        }
      }
    }
    ret
  }

  def containsFilesWithRecursiveCheck(hdfsLocation: String): Boolean = {
    val conf = sc.hadoopConfiguration
    val fs = FileSystem.get(conf)
    var ret = fs.exists(new Path(hdfsLocation))

    if (ret) {
      ret = false
      val iter = fs.listFiles(new Path(hdfsLocation), true)
      while (iter.hasNext()) {
        val f = iter.next().getPath().getName
        if (!f.equals("_SUCCESS")) {
          ret = true
        }
      }
    }
    ret
  }
  

import org.apache.hadoop.fs.{ FileSystem, Path }
  
    def cleanup(hdfsLocation: String): Unit = {
    if (!hdfsLocation.equals(Constants.BASE_DIR) &&
      !hdfsLocation.equals(Constants.PAYLOAD_DIR) &&
      !hdfsLocation.equals(Constants.OUTPUT_DIR)) {
      val conf = Utils.sc.hadoopConfiguration
      val fs = FileSystem.get(conf)
      if (fs.exists(new Path(hdfsLocation))) {
        fs.delete(new Path(hdfsLocation), true)
        Log.info("Destination Directory Found And Deleted : " + hdfsLocation)
        fs.close()
      }
    }
  }

  def isPathExist(hdfsLocation: String): Boolean = {
    val conf = Utils.sc.hadoopConfiguration
    val fs = FileSystem.get(conf)
    fs.exists(new Path(hdfsLocation))
  }

  def getRecentPartitionPath(rootLocation: String): String = {
    val conf = Utils.sc.hadoopConfiguration
    val fs = FileSystem.get(conf)
    var recentPartitonName = ""
    if (fs.exists(new Path(rootLocation))) {
      val fsList = fs.globStatus(new Path(rootLocation + "/*"))
      val fsSortedList = fsList.sortWith((fs1, fs2) => fs1.getModificationTime > fs2.getModificationTime)
      recentPartitonName = fsSortedList(0).getPath.getName
    }
    val finalPath = rootLocation + "/" + recentPartitonName + "/*"
    Log.info("Path -> " + finalPath)
    finalPath
  }
  
  
import org.apache.log4j.{ Level, Logger }
import java.util.Properties
import org.apache.commons.lang.StringUtils
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.fs.Path

Log.setLevel(Level.DEBUG)

  var properties: Properties = new Properties()

  def loadConfig(configPath: String) = {

    Log.info("Loading configuration from: " + configPath)

    val path = new Path(configPath)
    val fs = FileSystem.get(Utils.sc.hadoopConfiguration)
    val inputStream = fs.open(path)

    properties.load(inputStream)

    Log.info("Done loading configuration.")

  }

  def get(key: String): String = {
    if (properties == null)
      StringUtils.EMPTY
    else
      properties.getProperty(key)
  }

  def get(key: String, defValue: String): String = {
    if (properties == null)
      StringUtils.EMPTY

    val result = properties.getProperty(key)
    if (result == null)
      defValue
    else
      result
  }
  
  
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{ FileSystem, Path }
  
  def exists(path: String): Boolean = hdfs.exists(new Path(path))
  
 def deleteHDFSFolder(sc: SparkContext, hdfsLocation: String): Unit = {
    val conf = sc.hadoopConfiguration
    val fs = org.apache.hadoop.fs.FileSystem.get(conf)
    if (fs.exists(new org.apache.hadoop.fs.Path(hdfsLocation))) {
      fs.delete(new Path(hdfsLocation), true)
      Log.info("Destination Directory Found And Deleted : " + hdfsLocation)
      fs.close()
    }
  }


import org.apache.hadoop.fs.{ FileSystem, Path }
import org.apache.spark.SparkContext
import org.apache.spark.sql.{ Row, SQLContext }
import org.apache.spark.sql.types._

import scala.collection.mutable.ArrayBuffer

object FileProcessor {
  def run(opts: Map[String, String]): Unit = {
    val schemaPath = opts("schemaPath")
    val dataPath = opts("dataPath")
    val outputPath = opts("outputPath")
    val TimestampPattern = """[tT]imestamp\((.*)\)""".r
    val DatePattern = """[dD]ate\((.*)\)""".r
    val DecimalPattern = """[dD]ecimal\((\d+)\:(\d+)\)""".r
    val conf = Utils.sc.hadoopConfiguration
    val fs = FileSystem.get(conf)
    fs.delete(new Path(outputPath), true)

    val schemaFile = Utils.sc.textFile(schemaPath)
    val schemaLines = schemaFile.collect()
    val delimiter = schemaLines.find(line => line.startsWith("schema-opt-delimiter=")).map(_.split("=")(1)).getOrElse("|")
    val hasHeader = schemaLines.find(line => line.startsWith("schema-opt-header=")).map(_.split("=")(1).toBoolean).getOrElse(false)

    val filteredSchemaFile = schemaLines.filter(line => !line.startsWith("//") && !line.startsWith("schema-opt-"))

    val fields = filteredSchemaFile.map(line => {
      val tokens = line.split(",")
      tokens(1).toLowerCase match {
        case "integer" => StructField(tokens(0), IntegerType, true)
        case "long" => StructField(tokens(0), LongType, true)
        case "float" => StructField(tokens(0), FloatType, true)
        case "double" => StructField(tokens(0), DoubleType, true)
        case TimestampPattern(pattern) =>
          StructField(tokens(0), TimestampType, true, new MetadataBuilder().putString("pattern", pattern).build())
        case DatePattern(pattern) => StructField(tokens(0), DateType, true, new MetadataBuilder().putString("pattern", pattern).build())
        case DecimalPattern(p, s) => StructField(tokens(0), DecimalType(p.toInt, s.toInt), true)
        case _ => StructField(tokens(0), StringType, true)
      }
    })
    val schema = StructType(fields)
    val columnIndexs = filteredSchemaFile.map(line => {
      val toks = line.split(",")
      toks(2).toInt -> toks(1)
    })

    val inputDf = Utils.sc.textFile(dataPath)
    val header = inputDf.first()
    val dataDf = if (hasHeader) inputDf.filter(_ != header) else inputDf
    val mappedData = dataDf.map(x => {
      val cols = x.split("\\|")
      var rowVals: ArrayBuffer[Any] = ArrayBuffer.empty
      for (colInd <- columnIndexs) {
        if (cols.size > colInd._1 && cols(colInd._1) != null && !cols(colInd._1).trim.isEmpty && cols(colInd._1) != "null") {
          val colVal = cols(colInd._1)
          colInd._2 match {
            case "integer" | "Integer" => rowVals += colVal.toInt
            case "long" | "Long" => rowVals += colVal.toLong
            case "float" | "Float" => rowVals += colVal.toFloat
            case "double" | "Double" => rowVals += colVal.toDouble
            case TimestampPattern(pattern) =>
              val sdf = new SimpleDateFormat(pattern)
              sdf.setTimeZone(TimeZone.getTimeZone("GMT"))
              rowVals += new Timestamp(sdf.parse(colVal).getTime)
            case DatePattern(pattern) => rowVals += new SqlDate(new SimpleDateFormat(pattern).parse(colVal).getTime)
            case DecimalPattern(p, s) => rowVals += Decimal(BigDecimal(colVal), p.toInt, s.toInt)
            case _ => rowVals += cols(colInd._1)
          }
        } else {
          rowVals += null
        }
      }
      Row(rowVals: _*)
    })
    val resultDf = Utils.sparkSession.createDataFrame(mappedData, schema)
    resultDf.write.parquet(outputPath)
  }
}



import org.apache.hadoop.fs.{ FileSystem, Path, FileUtil }
import scala.collection.mutable.{ ListBuffer, ArrayBuffer }

object DeleteEmptyFiles {

  implicit def PathConverter(p: String): Path = new Path(p)

  override def processDays(days: ArrayBuffer[String]): Unit = {

    val dir = "/db/xyz/intermediary/flatfile2_errors_201610"
    deleteEmptyFiles(dir)

  }

  def deleteEmptyFiles(basePath: String): Unit = {
    val outputPathList = new ListBuffer[String]
    val conf = Utils.sc.hadoopConfiguration
    val fs = FileSystem.get(conf)
    val files = fs.listFiles(new Path(basePath), false)
    while (files.hasNext) {
      val srcPath = files.next.getPath.toString
      val size = fs.getFileStatus(srcPath).getLen
      println(size)
      if (size == 0) {
        outputPathList += srcPath
      }
    }
    for (path <- outputPathList) {
      fs.delete(path, true)
    }
  }
}




import org.apache.hadoop.fs.{ FileSystem, Path }
import org.apache.spark.sql.Row
import scala.collection.mutable.{ ArrayBuffer, ListBuffer }

  def getDateYMDpathFormat(day: String): String = {
    val year = day.substring(0, 4)
    val month = day.substring(4, 6)
    val date = day.substring(6, 8)
    s"$year/$month/$date/"
  }

  def orc2Text(sqlQuery: String, prefixColumns: List[String], partitionName: String, transformLineFn: String => String,
    outputDir: String, isPrefixOrPayload: String, includePrefixInPayload: Boolean): Unit = {
    val sqlContext = Utils.sparkSession.sqlContext
    //sqlContext.setConf("hive.input.dir.recursive", "true")
    //sqlContext.setConf("hive.supports.subdirectories", "true")
    import sqlContext.implicits._
    val table = Utils.sparkSession.sql(sqlQuery)
    println("\n<<< " + sqlQuery + " >>>\n")
    val partitionedTable = table.map(row => {
      val partitionDt = row.getAs[String](partitionName)
      var line = buildPipedLine(row)
      val parts = line.split("\\|")
      var prefixPart = ""
      var list_parts: (Array[String], Array[String]) = null
      var finalLine =
        if (isPrefixOrPayload.equalsIgnoreCase("PREFIX")) {
          if (!prefixColumns.isEmpty) {
            list_parts = parts.splitAt(prefixColumns.size)
            prefixPart = list_parts._1.mkString("", ",", "")
          }
          prefixPart
        } else {
          list_parts = if (!prefixColumns.isEmpty && includePrefixInPayload) parts.splitAt(prefixColumns.size) else parts.splitAt(-1)
          prefixPart = list_parts._1.mkString("", ",", "")
          val payloadPart = list_parts._2.mkString("", "|", "")
          val transformedPayload = if (transformLineFn != null) transformLineFn(payloadPart) else payloadPart
          if (includePrefixInPayload) prefixPart + "|" + transformedPayload else transformedPayload
        }
      (partitionDt, finalLine)
    }).filter(_._1 != "__HIVE_DEFAULT_PARTITION__").toDF("partition", "line")
    cleanup(outputDir)
    partitionedTable.write.partitionBy("partition").text(outputDir)
  }

  def buildPipedLine(rawLine: Row): String = {
    val record_line = rawLine.mkString("#^^#").replaceAll("[^\\x00-\\x7F]", "-").replaceAll("null", "")
      .replaceAll("\"", "").replaceAll("\\|", "")
      .replaceAll("#\\^\\^#", "|")
    record_line
  }

  def renameOutputDir(basePath: String, all: Boolean): Unit = {
    val outputPathList = new ListBuffer[String]
    val conf = Utils.sc.hadoopConfiguration
    val fs = FileSystem.get(conf)
    fs.delete(new Path(basePath + "_SUCCESS"), false)
    val files = fs.listFiles(new Path(basePath), true)
    while (files.hasNext) {
      val srcPath = files.next.getPath.toString
      val index = srcPath.indexOf("partition=")
      val fileName = srcPath.substring(index + 21)
      val day = srcPath.substring(index + 10, index + 20)
      val year = day.substring(0, 4)
      val month = day.substring(5, 7)
      val date = day.substring(8, 10)
      val destPath = if (all) basePath + year + "/" + month + "/" + date else basePath
      //println("SRC -> " + srcPath)
      //println("DST -> " + destPath)
      fs.mkdirs(new Path(destPath))
      fs.rename(new Path(srcPath), new Path(destPath + "/" + fileName))
      outputPathList += basePath + "partition=" + day
    }
    for (path <- outputPathList) {
      fs.delete(new Path(path), true)
    }
  }
  
 
scalacOptions ++=
  Seq("-encoding", "UTF8", "-unchecked", "-deprecation", "-language:postfixOps", "-language:implicitConversions", "-language:higherKinds", "-language:reflectiveCalls")


val sparkVersion = "2.1.0"

libraryDependencies ++= Seq("org.apache.spark" %% "spark-hive" % sparkVersion,
  "org.apache.spark" %% "spark-sql" % sparkVersion,
  "org.apache.spark" %% "spark-yarn" % sparkVersion,
  "org.spark-project.hive" % "hive-metastore" % "1.2.1.spark2",
  "org.slf4j" % "slf4j-api" % "1.7.21",

  "org.scalatest" %% "scalatest" % "3.0.0",
  "com.holdenkarau" %% "spark-testing-base" % "2.0.0_0.4.4",
  "com.github.scopt" %% "scopt" % "3.5.0",
  "org.scalaz"  %% "scalaz-core" % "7.2.7",
  "org.scalactic" %% "scalactic" % "3.0.1"
  


  
