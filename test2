scalacOptions ++=
  Seq("-encoding", "UTF8", "-unchecked", "-deprecation", "-language:postfixOps", "-language:implicitConversions", "-language:higherKinds", "-language:reflectiveCalls")
resolvers += "Hortonworks Releases" at "http://repo.hortonworks.com/content/repositories/releases/"

libraryDependencies ++= Seq("org.apache.spark" %% "spark-hive" % sparkVersion,
  "org.apache.spark" %% "spark-sql" % sparkVersion,
  "org.apache.spark" %% "spark-yarn" % sparkVersion,
  "org.spark-project.hive" % "hive-metastore" % "1.2.1.spark2",
  "org.slf4j" % "slf4j-api" % "1.7.21",

  "org.scalatest" %% "scalatest" % "3.0.0",
  "com.holdenkarau" %% "spark-testing-base" % "2.0.0_0.4.4",
  "com.github.scopt" %% "scopt" % "3.5.0",
  "org.scalaz"  %% "scalaz-core" % "7.2.7",
  "org.scalactic" %% "scalactic" % "3.0.1",
    "com.typesafe" % "config" % "1.3.1"
)

//fork in run := true

//mainClass in assembly := Some("com.xyz.Driver")

plugins.sbt:

logLevel := Level.Warn

addSbtPlugin("com.eed3si9n"       % "sbt-buildinfo"   % "0.3.2")
addSbtPlugin("org.scalariform"    % "sbt-scalariform" % "1.6.0")
addSbtPlugin("com.eed3si9n"       % "sbt-assembly"    % "0.14.2")





import org.apache.log4j.{ Level, Logger }
import java.util.Properties

import org.apache.commons.lang.StringUtils
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.fs.Path

Config.loadConfig(jobConfig.configPath)

object Config {

  val Log = Logger.getLogger("Config")
  Log.setLevel(Level.DEBUG)

  var properties: Properties = new Properties()

  def loadConfig(configPath: String) = {

    Log.info("Loading configuration from: " + configPath)

    val path = new Path(configPath)
    val fs = FileSystem.get(sc.hadoopConfiguration)
    val inputStream = fs.open(path)

    properties.load(inputStream)

    Log.info("Done loading configuration.")

  }

  def get(key: String): String = {
    if (properties == null)
      StringUtils.EMPTY
    else
      properties.getProperty(key)
  }

  def get(key: String, defValue: String): String = {
    if (properties == null)
      StringUtils.EMPTY

    val result = properties.getProperty(key)
    if (result == null)
      defValue
    else
      result
  }
}

  val CONSTANT1 = Config.get("table.constant1")
  val CONSTANT2 = Config.get("table.constant2")

  val CONSTANT3 = Config.get("file.constant3")
  val CONSTANT4 = Config.get("file.constant4")
  
------------------  
import com.typesafe.config.{ Config, ConfigFactory }

val config: Config = loadConfig(sparkInitFile)

    val sparkValues = config.entrySet().collect {
      case e if e.getKey.startsWith("spark") => e.getKey -> e.getValue.unwrapped().toString
    }.toMap

    val conf = new SparkConf()
      .setAppName(appName).setAll(sparkValues)
	  
    val master = sparkMaster match {
      case "local" => "local"
      case "yarncluster" | "yarn-cluster" => "yarn-cluster"
      case "yarnclient" | "yarn-client" => "yarn-client"
      case _ => sparkMaster
    }

    conf.setMaster(master)
	
	    Log.info("Config")

    val allconf = conf.getAll
    allconf.foreach(c => {
      Log.info(c._1 + " = " + c._2)
    })
	
  private def loadConfig(sparkInitFile: Option[String]): Config = {
    val config: Config = sparkInitFile.map { path =>
      val file = new File(path)
      if (!file.exists()) {
        throw new Exception(s"Config path ${file.getAbsolutePath} doesn't exist!")
      }

      Log.info(s"Loading properties from ${file.getAbsolutePath}")
      ConfigFactory.load(ConfigFactory.parseFile(file).resolve())
    }.getOrElse {
      ConfigFactory.load()
    }
    config
  }
------------------

log4j.xml:
<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE log4j:configuration SYSTEM "log4j.dtd">
<log4j:configuration debug="true"
                     xmlns:log4j='http://jakarta.apache.org/log4j/'>

    <appender name="console" class="org.apache.log4j.ConsoleAppender">
        <layout class="org.apache.log4j.PatternLayout">
            <param name="ConversionPattern"
                   value="%d{yyyy-MM-dd HH:mm:ss} %-5p %c{3}:%l - %m%n" />
        </layout>
    </appender>

    <appender name="fileappender" class="org.apache.log4j.RollingFileAppender">
        <param name="append" value="false" />
        <param name="maxFileSize" value="10MB" />
        <param name="maxBackupIndex" value="10" />
        <param name="File" value="/home/nmalli/proj/logs/application.log" />
        <layout class="org.apache.log4j.PatternLayout">
            <param name="ConversionPattern"
                   value="%d{yyyy-MM-dd HH:mm:ss} %-5p %c{3}:%l - %m%n" />
        </layout>
    </appender>

    <root>
        <level value="INFO" />
        <appender-ref ref="console" />
        <appender-ref ref="fileappender" />
    </root>

</log4j:configuration>

-------------------------------

#Spark properties
spark.eventLog.enabled = false
spark.eventLog.dir = /tmp
spark.scheduler.mode = FAIR
spark.memory.useLegacyMode = false
spark.yarn.executor.memoryOverhead = 4096
spark.network.timeout = 1200
spark.rpc.askTimeout = 1200
spark.rpc.lookupTimeout = 1200
spark.executor.heartbeatInterval = 30
spark.rpc.numRetries = 6
spark.task.maxFailures = 8
spark.sql.shuffle.partitions = 201
-------------

cd /home/xxx/jars
export SPARK_JARS2=ssl-config-akka_2.11-0.2.1.jar,ssl-config-core_2.11-0.2.1.jar,config-1.3.0.jar

STARTDAY=`date +"%Y%m%d"`
ENDDAY=`date +"%Y%m%d"`
JOB=xyz
EXECUTORS_NUM=10
EXECUTORS_MEM=4G

LOGNAME=$JOB\_`date +"%Y%m%d_%H_%M_%S"`
XYZLOG=/home/xxx/logs/$LOGNAME.log
echo "Submitting the Spark job to the YARN"
/spark2-submit --verbose --master yarn --deploy-mode client --conf spark.yarn.submit.waitAppCompletion=true --queue=&QUEUE --num-executors $EXECUTORS_NUM  --executor-memory $EXECUTORS_MEM --executor-cores 1 --driver-memory 8G  --class com.xyz.Driver --jars $SPARK_JARS2 /home/xxx/jars/xyz-0.1.jar --config=/db/xxx/xyz.config --appName=XYZ --sparkMaster=yarnclient --job=$JOB --days=$STARTDAY,$ENDDAY >> $XYZLOG 2>&1
ret=${?}
if [ ${ret} -ne 0 ]
then
echo "Error - Please check the log file"
exit ${ret}
else
echo "Job completed successfully"
exit 0
fi


nohup spark2-submit --verbose --master yarn --deploy-mode client --queue=queue_name --num-executors 10  --executor-memory 4G --executor-cores 1 --driver-memory 8G  --class com.xyz.Driver /home/xxx/xyz_2.10-1.0.jar appName=XYZ sparkMaster=yarnclient inifile=/home/xxx/ini/CTP_Summary_Reports.ini job=JOB_NAME >> /home/xxx/log/test.log 2>&1&

----------------------
