COHUTTA,3/14/14,23:57,9.88,3.81,1043,1.73,51,1.47
COHUTTA,3/14/14,23:59,9.9,3.795,1039,1.48,50,1.11
NANTAHALLA,3/10/14,1:01,10.47,1.712,778,1.96,76,0.78
NANTAHALLA,3/10/14,1:02,10.41,1.713,779,1.68,82,1.57

------------------
name := "ReadHbase"

version := "1.0"

//scalaVersion := "2.11.8"
scalaVersion := "2.10.6"




// https://mvnrepository.com/artifact/org.apache.hbase/hbase-client
libraryDependencies += "org.apache.hbase" % "hbase-client" % "1.2.0"
// https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10
libraryDependencies += "org.apache.spark" % "spark-core_2.10" % "1.6.1"
// https://mvnrepository.com/artifact/org.apache.spark/spark-streaming_2.10
libraryDependencies += "org.apache.spark" % "spark-streaming_2.10" % "1.6.1"

// https://mvnrepository.com/artifact/org.apache.spark/spark-sql_2.10
libraryDependencies += "org.apache.spark" % "spark-sql_2.10" % "1.6.1"

// https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.11
//libraryDependencies += "org.apache.spark" % "spark-core_2.11" % "2.0.2"

// https://mvnrepository.com/artifact/org.apache.spark/spark-streaming_2.11
//libraryDependencies += "org.apache.spark" % "spark-streaming_2.11" % "2.0.2"





// https://mvnrepository.com/artifact/org.apache.hbase/hbase-annotations
libraryDependencies += "org.apache.hbase" % "hbase-annotations" % "1.2.0"
// https://mvnrepository.com/artifact/org.apache.hbase/hbase-common
libraryDependencies += "org.apache.hbase" % "hbase-common" % "1.2.0"
// https://mvnrepository.com/artifact/org.apache.hbase/hbase-protocol
libraryDependencies += "org.apache.hbase" % "hbase-protocol" % "1.2.0"
// https://mvnrepository.com/artifact/org.apache.htrace/htrace-core

libraryDependencies += "org.apache.zookeeper" % "zookeeper" % "3.4.8"

// https://mvnrepository.com/artifact/org.apache.hbase/hbase
libraryDependencies += "org.apache.hbase" % "hbase" % "1.2.0"

// https://mvnrepository.com/artifact/org.apache.hbase/hbase-server
libraryDependencies += "org.apache.hbase" % "hbase-server" % "1.2.0" excludeAll ExclusionRule(organization = "org.mortbay.jetty")
// https://mvnrepository.com/artifact/org.apache.hbase/hbase-procedure
libraryDependencies += "org.apache.hbase" % "hbase-procedure" % "1.2.0"
// https://mvnrepository.com/artifact/org.apache.hbase/hbase-hadoop-compat
libraryDependencies += "org.apache.hbase" % "hbase-hadoop-compat" % "1.2.0"

libraryDependencies += "it.nerdammer.bigdata" % "spark-hbase-connector_2.10" % "1.0.3"


------------------------------

package edu.XYZ.sparkhbase.write

import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapred.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.mapred.JobConf
import org.apache.log4j.{Level, Logger}
import org.apache.spark.{SparkConf, SparkContext}

case class Sensor(resid: String, date: String, time: String, hz: Double, disp: Double, flo: Double,
                  sedPPM: Double, psi: Double, chlPPM: Double) {
  override def toString: String = {
    resid + "," + date + "," + time + "," + hz + "," + disp + "," + flo + "," + sedPPM + "," + "," + psi + "," + chlPPM
  }
}

object HbaseWriteSpark {

  final val cfDataBytes = Bytes.toBytes("data")
  final val cfAlertBytes = Bytes.toBytes("alert")
  final val colHzBytes = Bytes.toBytes("hz")
  final val colDispBytes = Bytes.toBytes("disp")
  final val colFloBytes = Bytes.toBytes("flo")
  final val colSedBytes = Bytes.toBytes("sedPPM")
  final val colPsiBytes = Bytes.toBytes("psi")
  final val colChlBytes = Bytes.toBytes("chlPPM")

  def main(args: Array[String]): Unit = {

    Logger.getLogger("org").setLevel(Level.ERROR)
    val sparkConf = new SparkConf().setAppName("WritetoHBaseFromSpark").setMaster("local[*]")

    val sc = new SparkContext(sparkConf)
    val data = sc.textFile("./data/Sensor_data.txt")
    println(data.count())

    val sensordata = data.map(parseSensor)
    sensordata.take(2).foreach(println)

    val alertRDD = sensordata.filter(sensor => sensor.psi < 5.0)
    sensordata.map(convertToPut).
      saveAsHadoopDataset(getHbaseConf)

    alertRDD.map(convertToPutAlert).saveAsHadoopDataset(getHbaseConf)



  }


  def convertToPutAlert(sensor: Sensor): (ImmutableBytesWritable, Put) = {
    val dateTime = sensor.date + " " + sensor.time
    // create a composite row key: sensorid_date time
    val key = sensor.resid + "_" + dateTime
    val p = new Put(Bytes.toBytes(key))
    // add to column family alert, column psi data value to put object
    p.addColumn(cfAlertBytes, colPsiBytes, Bytes.toBytes(sensor.psi))
    return (new ImmutableBytesWritable(Bytes.toBytes(key)), p)
  }


  def parseSensor(str: String): Sensor = {
    val p = str.split(",")
    Sensor(p(0), p(1), p(2), p(3).toDouble, p(4).toDouble, p(5).toDouble, p(6).toDouble,
      p(7).toDouble, p(8).toDouble)
  }

  def convertToPut(sensor: Sensor): (ImmutableBytesWritable, Put) = {
    val rowkey = sensor.resid + "_" + sensor.date + "_" + sensor.time
    val put = new Put(Bytes.toBytes(rowkey))
    put.addColumn(cfDataBytes, colHzBytes, Bytes.toBytes(sensor.hz))
    put.addColumn(cfDataBytes, colDispBytes, Bytes.toBytes(sensor.disp))
    put.addColumn(cfDataBytes, colFloBytes, Bytes.toBytes(sensor.flo))
    put.addColumn(cfDataBytes, colSedBytes, Bytes.toBytes(sensor.sedPPM))
    put.addColumn(cfDataBytes, colPsiBytes, Bytes.toBytes(sensor.psi))
    put.addColumn(cfDataBytes, colChlBytes, Bytes.toBytes(sensor.chlPPM))
    return (new ImmutableBytesWritable(Bytes.toBytes(rowkey)), put)
  }

  def getHbaseConf(): JobConf = {
    val conf = HBaseConfiguration.create()
    conf.set("hbase.zookeeper.quorum", "XYZ.net,XYZ.net,XYZ.net")
    conf.set("hbase.zookeeper.property.clientPort", "2181")
    conf.set("zookeeper.znode.parent", "/hbase")

    val jobConfig: JobConf = new JobConf(conf,
      this.getClass)


    jobConfig.setOutputFormat(classOf[TableOutputFormat])
    jobConfig.set(TableOutputFormat.OUTPUT_TABLE,
      "sensor_data")

    return jobConfig
  }


}


---------------------------

package com.XYZ.timeline.hbaseread

import org.json4s._
import org.json4s.jackson.JsonMethods._
import org.json4s.JsonDSL._

import scala.io.Source
import java.net._
import java.util

import com.google.common.io.BaseEncoding


object HbaseRestApiParser {

  def main(args: Array[String]): Unit = {

   val url ="http://XYZ.net:20550/cxeTimeline/8499100100859338/"
   val rp = Map("Accept" -> "application/json")
   val connection = new URL(url).openConnection()
    connection.setRequestProperty("Accept","application/json")
   val response = Source.fromInputStream(connection.getInputStream()).mkString

    val jvalue = parse(response)

    val key = (jvalue \ "Row" \ "key").values.toString


  val decoded = new String(BaseEncoding.base64().decode(key))
    println(decoded)

    //println((jvalue \ "Row" \ "Cell").values)

    //println(jvalue


     var cells = (jvalue \ "Row" \ "Cell" \ "column").values

       println(cells)
  }
}
-------------------------
package com.XYZ.timeline.hbaseread

import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.log4j.{Level, Logger}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.hadoop.hbase.util.Bytes

object ReadHbaseHadoopApi extends Serializable {
  def main(args: Array[String]): Unit = {

    Logger.getLogger("org").setLevel(Level.ERROR)
    val sparkConf = new SparkConf().setAppName("ReadHbaseHadoopApi").setMaster("local[*]")


    val sc = new SparkContext(sparkConf)

    val conf = HBaseConfiguration.create()
    conf.set("hbase.zookeeper.quorum", "XYZ.net,XYZ.net,XYZ.net")
    conf.set("hbase.zookeeper.property.clientPort", "2181")
    conf.set("zookeeper.znode.parent", "/hbase")
    //conf.set("hbase.rpc.timeout", "3000")
    conf.set(TableInputFormat.INPUT_TABLE, "sensor_data")
    //conf.set(TableInputFormat.SCAN_ROW_START, "1")
    //conf.set(TableInputFormat.SCAN_ROW_STOP, "40")
    // specify specific column to return
    //conf.set(TableInputFormat.SCAN_COLUMNS, "cf2")

    val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result])

    println(hBaseRDD.count())
    //hBaseRDD.foreach(x=>println("rowkey in string" + Bytes.toString(x._1.get) ))

    val resultRDD = hBaseRDD.map(tuple => tuple._2)


    //val keyValueRDD = resultRDD.map(result => (Bytes.toString(result.getRow()).split(",")(0), Bytes.toDouble(result.value)))

    //val keyValueRDD = resultRDD.map(result => Bytes.toString(result.value()))

    resultRDD.foreach{result=>
      println("Row->"+ Bytes.toString(result.getRow()))
      println(" value->" + Bytes.toDouble(result.getValue("data".getBytes(),"psi".getBytes())))}

  }
}

----------------------------
package com.XYZ.timeline.hbaseread


import org.apache.spark._

import it.nerdammer.spark.hbase._

object ReadHbaseWithConnector {
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setAppName("ReadTimelineHbaseConnector").setMaster("local")
    sparkConf.set("spark.hbase.host", "XYZ.net,XYZ.net,XYZ.net")
    val sc = new SparkContext(sparkConf)
    val colFam = "acct"

    val hBaseRDD = sc.hbaseTable[(String, Int, String)]("cxeTimeline").select("*").inColumnFamily(colFam)
      .withStartRow("8518367685336390").withStopRow("8532339519003110")
      print("RDD Count" + hBaseRDD.count())

  }
}

---------------------------

package com.XYZ.timeline.hbaseread

import org.apache.hadoop.hbase.client.{HBaseAdmin, Result}
import org.apache.hadoop.hbase.{ HBaseConfiguration, HTableDescriptor }
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.spark._

object ReadTimeline {

  def main(args: Array[String]): Unit = {

    val sparkConf = new SparkConf().setAppName("ReadTimelineHbase").setMaster("local")
    val sc = new SparkContext(sparkConf)
    val conf = HBaseConfiguration.create()
    val tableName = "cxeTimeline"

    conf.set("hbase.master", "ebdp-avdc-d365p.sys.XYZ.net:60010")
    conf.setInt("timeout", 120000)
    conf.set("hbase.zookeeper.quorum", "XYZ.net,XYZ.net,XYZ.net")
    conf.set("zookeeper.znode.parent", "/hbase")
    conf.set(TableInputFormat.INPUT_TABLE, tableName)

    val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result])
    val resultRdd = hBaseRDD.values
    println( hBaseRDD.map(k => k.toString()).first())

    sc.stop()

  }

}
